{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = 10\n",
    "input_size = 784\n",
    "epochs = 2\n",
    "learning_rate = 0.01\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Lambda(lambda x: x.repeat(3, 1, 1)) # gray channel to rgb          \n",
    "])\n",
    "train_datasets = torchvision.datasets.MNIST(root='./datasets/', train=True, transform=transform)\n",
    "test_datasets = torchvision.datasets.MNIST(root='./datasets/', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_datasets, batch_size=batch_size, shuffle=True,)\n",
    "test_loader = DataLoader(test_datasets, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "# freeze params\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# change 3 channel to single channel\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7,7), stride=(2,2), padding=(3,3), bias=False)\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 2, step 100 / 600, loss = 0.9667\n",
      "epoch 1 / 2, step 200 / 600, loss = 0.6079\n",
      "epoch 1 / 2, step 300 / 600, loss = 0.9426\n",
      "epoch 1 / 2, step 400 / 600, loss = 0.3553\n",
      "epoch 1 / 2, step 500 / 600, loss = 0.6085\n",
      "epoch 1 / 2, step 600 / 600, loss = 0.5233\n",
      "epoch 2 / 2, step 100 / 600, loss = 0.6308\n",
      "epoch 2 / 2, step 200 / 600, loss = 0.7595\n",
      "epoch 2 / 2, step 300 / 600, loss = 0.7638\n",
      "epoch 2 / 2, step 400 / 600, loss = 0.6811\n",
      "epoch 2 / 2, step 500 / 600, loss = 0.5391\n",
      "epoch 2 / 2, step 600 / 600, loss = 0.5764\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "total_steps = len(train_loader)\n",
    "for epoch in range(epochs):\n",
    "    for idx, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # forward\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # update weight\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if (idx+1) % 100 == 0:\n",
    "            print(f\"epoch {epoch + 1} / {epochs}, step {idx+1} / {total_steps}, loss = {loss:.4f}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 86.78\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_corrects = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        n_samples += labels.shape[0] \n",
    "        n_corrects += (predictions == labels).sum().item()\n",
    "    \n",
    "    acc = n_corrects / n_samples * 100.0\n",
    "    print(f'accuracy = {acc:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "22d82af33f03da296223e6f809e18aa0877c44b8d18707f5a9076a6c9918bbd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
