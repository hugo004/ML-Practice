{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size: int, num_classes: int) -> None:\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, num_classes)\n",
    "        \n",
    "    def forward(self, x: Tensor):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "batch_size = 64\n",
    "num_epochs = 1\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 11532870.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 14704502.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 11122927.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 10342306.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset  = datasets.MNIST(root='./datasets', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset  = datasets.MNIST(root='./datasets', train=False, transform=transforms.ToTensor(), download=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(input_size=input_size, num_classes=num_classes).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.rand((64, 1, 28, 28))\n",
    "test.reshape(test.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.12298229336738586\n",
      "loss: 0.12354741990566254\n",
      "loss: 0.12160138785839081\n",
      "loss: 0.11580488830804825\n",
      "loss: 0.14661163091659546\n",
      "loss: 0.11438895761966705\n",
      "loss: 0.08118384331464767\n",
      "loss: 0.09254367649555206\n",
      "loss: 0.23436498641967773\n",
      "loss: 0.19459864497184753\n",
      "loss: 0.14921513199806213\n",
      "loss: 0.07203206419944763\n",
      "loss: 0.19980569183826447\n",
      "loss: 0.12475406378507614\n",
      "loss: 0.08285287767648697\n",
      "loss: 0.1534738689661026\n",
      "loss: 0.12209007889032364\n",
      "loss: 0.06759630888700485\n",
      "loss: 0.080714151263237\n",
      "loss: 0.1392904371023178\n",
      "loss: 0.1911219358444214\n",
      "loss: 0.08377555012702942\n",
      "loss: 0.14705750346183777\n",
      "loss: 0.0856703445315361\n",
      "loss: 0.20958732068538666\n",
      "loss: 0.24413008987903595\n",
      "loss: 0.1477138251066208\n",
      "loss: 0.05085792392492294\n",
      "loss: 0.2195497304201126\n",
      "loss: 0.100023053586483\n",
      "loss: 0.07757953554391861\n",
      "loss: 0.026892930269241333\n",
      "loss: 0.2984291911125183\n",
      "loss: 0.10246266424655914\n",
      "loss: 0.19634780287742615\n",
      "loss: 0.07858657091856003\n",
      "loss: 0.10796257853507996\n",
      "loss: 0.0961274728178978\n",
      "loss: 0.10997599363327026\n",
      "loss: 0.09431430697441101\n",
      "loss: 0.06625151634216309\n",
      "loss: 0.1441638171672821\n",
      "loss: 0.1380666047334671\n",
      "loss: 0.11293376982212067\n",
      "loss: 0.2346591204404831\n",
      "loss: 0.057232655584812164\n",
      "loss: 0.16128699481487274\n",
      "loss: 0.13436754047870636\n",
      "loss: 0.11743862181901932\n",
      "loss: 0.07127925008535385\n",
      "loss: 0.05744510889053345\n",
      "loss: 0.1657840460538864\n",
      "loss: 0.1319509893655777\n",
      "loss: 0.10644760727882385\n",
      "loss: 0.17672087252140045\n",
      "loss: 0.05510914325714111\n",
      "loss: 0.15888246893882751\n",
      "loss: 0.15428979694843292\n",
      "loss: 0.11119671911001205\n",
      "loss: 0.177093043923378\n",
      "loss: 0.1430533081293106\n",
      "loss: 0.12071425467729568\n",
      "loss: 0.05795208364725113\n",
      "loss: 0.13072149455547333\n",
      "loss: 0.22354651987552643\n",
      "loss: 0.10831639915704727\n",
      "loss: 0.08625226467847824\n",
      "loss: 0.09864643961191177\n",
      "loss: 0.2673228085041046\n",
      "loss: 0.12155412137508392\n",
      "loss: 0.07792478799819946\n",
      "loss: 0.030476529151201248\n",
      "loss: 0.18287166953086853\n",
      "loss: 0.10642268508672714\n",
      "loss: 0.10972928255796432\n",
      "loss: 0.1800720989704132\n",
      "loss: 0.11698934435844421\n",
      "loss: 0.1923271268606186\n",
      "loss: 0.109934501349926\n",
      "loss: 0.14207260310649872\n",
      "loss: 0.23688068985939026\n",
      "loss: 0.13294467329978943\n",
      "loss: 0.134242445230484\n",
      "loss: 0.1417979598045349\n",
      "loss: 0.07298597693443298\n",
      "loss: 0.21720606088638306\n",
      "loss: 0.16705530881881714\n",
      "loss: 0.06045213341712952\n",
      "loss: 0.042779259383678436\n",
      "loss: 0.21835029125213623\n",
      "loss: 0.1509326696395874\n",
      "loss: 0.1039532795548439\n",
      "loss: 0.10873390734195709\n",
      "loss: 0.17803232371807098\n",
      "loss: 0.06307463347911835\n",
      "loss: 0.15043923258781433\n",
      "loss: 0.10264914482831955\n",
      "loss: 0.1179201677441597\n",
      "loss: 0.10132122784852982\n",
      "loss: 0.19323456287384033\n",
      "loss: 0.2028273195028305\n",
      "loss: 0.13735592365264893\n",
      "loss: 0.08093152940273285\n",
      "loss: 0.2785426676273346\n",
      "loss: 0.08575864136219025\n",
      "loss: 0.24451282620429993\n",
      "loss: 0.11402612924575806\n",
      "loss: 0.13506464660167694\n",
      "loss: 0.14129523932933807\n",
      "loss: 0.13273820281028748\n",
      "loss: 0.1561092585325241\n",
      "loss: 0.34983956813812256\n",
      "loss: 0.09701114147901535\n",
      "loss: 0.11909976601600647\n",
      "loss: 0.13351422548294067\n",
      "loss: 0.04599472135305405\n",
      "loss: 0.07184303551912308\n",
      "loss: 0.20544733107089996\n",
      "loss: 0.23596294224262238\n",
      "loss: 0.11379872262477875\n",
      "loss: 0.19570961594581604\n",
      "loss: 0.07462548464536667\n",
      "loss: 0.10190466791391373\n",
      "loss: 0.057612258940935135\n",
      "loss: 0.15665705502033234\n",
      "loss: 0.16837415099143982\n",
      "loss: 0.2328978180885315\n",
      "loss: 0.06658118218183517\n",
      "loss: 0.2796160876750946\n",
      "loss: 0.08533110469579697\n",
      "loss: 0.0479268915951252\n",
      "loss: 0.11164851486682892\n",
      "loss: 0.13965682685375214\n",
      "loss: 0.3600921332836151\n",
      "loss: 0.02254899963736534\n",
      "loss: 0.0569174662232399\n",
      "loss: 0.14964666962623596\n",
      "loss: 0.09622061997652054\n",
      "loss: 0.18532133102416992\n",
      "loss: 0.17159679532051086\n",
      "loss: 0.16152171790599823\n",
      "loss: 0.0697234719991684\n",
      "loss: 0.20129665732383728\n",
      "loss: 0.27848395705223083\n",
      "loss: 0.21987752616405487\n",
      "loss: 0.30337533354759216\n",
      "loss: 0.22830800712108612\n",
      "loss: 0.1351020783185959\n",
      "loss: 0.08577274531126022\n",
      "loss: 0.10970601439476013\n",
      "loss: 0.0813901424407959\n",
      "loss: 0.102231465280056\n",
      "loss: 0.1950908899307251\n",
      "loss: 0.1990668922662735\n",
      "loss: 0.10155045986175537\n",
      "loss: 0.07021009922027588\n",
      "loss: 0.0592036135494709\n",
      "loss: 0.11223608255386353\n",
      "loss: 0.1958175003528595\n",
      "loss: 0.040957216173410416\n",
      "loss: 0.24200835824012756\n",
      "loss: 0.12775662541389465\n",
      "loss: 0.14264722168445587\n",
      "loss: 0.291283518075943\n",
      "loss: 0.12973622977733612\n",
      "loss: 0.12183160334825516\n",
      "loss: 0.18800853192806244\n",
      "loss: 0.09666244685649872\n",
      "loss: 0.22033463418483734\n",
      "loss: 0.1238054558634758\n",
      "loss: 0.31994372606277466\n",
      "loss: 0.13372258841991425\n",
      "loss: 0.08457399904727936\n",
      "loss: 0.16380228102207184\n",
      "loss: 0.05363774672150612\n",
      "loss: 0.23942425847053528\n",
      "loss: 0.05197771638631821\n",
      "loss: 0.176351398229599\n",
      "loss: 0.08568207919597626\n",
      "loss: 0.1344311535358429\n",
      "loss: 0.2154158651828766\n",
      "loss: 0.08274946361780167\n",
      "loss: 0.11192172765731812\n",
      "loss: 0.08338843286037445\n",
      "loss: 0.11734774708747864\n",
      "loss: 0.1643633395433426\n",
      "loss: 0.11835046112537384\n",
      "loss: 0.138637974858284\n",
      "loss: 0.08002424240112305\n",
      "loss: 0.09606035053730011\n",
      "loss: 0.13471341133117676\n",
      "loss: 0.12249094247817993\n",
      "loss: 0.22706212103366852\n",
      "loss: 0.12289851158857346\n",
      "loss: 0.07268285751342773\n",
      "loss: 0.09054458886384964\n",
      "loss: 0.04830360412597656\n",
      "loss: 0.12991157174110413\n",
      "loss: 0.1976698637008667\n",
      "loss: 0.18312883377075195\n",
      "loss: 0.15740959346294403\n",
      "loss: 0.030724698677659035\n",
      "loss: 0.07537660747766495\n",
      "loss: 0.1568228006362915\n",
      "loss: 0.13658879697322845\n",
      "loss: 0.04788085073232651\n",
      "loss: 0.3094538152217865\n",
      "loss: 0.1367410272359848\n",
      "loss: 0.07757209986448288\n",
      "loss: 0.06412320584058762\n",
      "loss: 0.15509633719921112\n",
      "loss: 0.34033864736557007\n",
      "loss: 0.11816049367189407\n",
      "loss: 0.15724819898605347\n",
      "loss: 0.07507838308811188\n",
      "loss: 0.0651470497250557\n",
      "loss: 0.14191347360610962\n",
      "loss: 0.03239021077752113\n",
      "loss: 0.175948366522789\n",
      "loss: 0.08146322518587112\n",
      "loss: 0.25761106610298157\n",
      "loss: 0.21794752776622772\n",
      "loss: 0.04361477494239807\n",
      "loss: 0.21031105518341064\n",
      "loss: 0.06782146543264389\n",
      "loss: 0.07710190117359161\n",
      "loss: 0.038803670555353165\n",
      "loss: 0.26780301332473755\n",
      "loss: 0.11388044059276581\n",
      "loss: 0.06604718416929245\n",
      "loss: 0.19333139061927795\n",
      "loss: 0.15647314488887787\n",
      "loss: 0.1620904803276062\n",
      "loss: 0.14211244881153107\n",
      "loss: 0.08992937207221985\n",
      "loss: 0.09882495552301407\n",
      "loss: 0.1302197128534317\n",
      "loss: 0.09435444325208664\n",
      "loss: 0.10923469811677933\n",
      "loss: 0.12321235239505768\n",
      "loss: 0.30259770154953003\n",
      "loss: 0.1473839432001114\n",
      "loss: 0.21110320091247559\n",
      "loss: 0.07482834160327911\n",
      "loss: 0.12270722538232803\n",
      "loss: 0.15026269853115082\n",
      "loss: 0.1801953762769699\n",
      "loss: 0.1885005533695221\n",
      "loss: 0.1121777668595314\n",
      "loss: 0.32763829827308655\n",
      "loss: 0.14390210807323456\n",
      "loss: 0.28471240401268005\n",
      "loss: 0.1257011741399765\n",
      "loss: 0.18098418414592743\n",
      "loss: 0.07719846814870834\n",
      "loss: 0.11446946114301682\n",
      "loss: 0.11486805230379105\n",
      "loss: 0.226305291056633\n",
      "loss: 0.18764446675777435\n",
      "loss: 0.07781241834163666\n",
      "loss: 0.10421109199523926\n",
      "loss: 0.04469124600291252\n",
      "loss: 0.11336052417755127\n",
      "loss: 0.18989384174346924\n",
      "loss: 0.15362979471683502\n",
      "loss: 0.08176916837692261\n",
      "loss: 0.16329458355903625\n",
      "loss: 0.09078776836395264\n",
      "loss: 0.0950636938214302\n",
      "loss: 0.11868341267108917\n",
      "loss: 0.09842213988304138\n",
      "loss: 0.0775187611579895\n",
      "loss: 0.26050880551338196\n",
      "loss: 0.06674487888813019\n",
      "loss: 0.12076899409294128\n",
      "loss: 0.1915651559829712\n",
      "loss: 0.08223572373390198\n",
      "loss: 0.08826208114624023\n",
      "loss: 0.10272463411092758\n",
      "loss: 0.09622460603713989\n",
      "loss: 0.08536021411418915\n",
      "loss: 0.08437101542949677\n",
      "loss: 0.11099454760551453\n",
      "loss: 0.11818855255842209\n",
      "loss: 0.08136714994907379\n",
      "loss: 0.10125210881233215\n",
      "loss: 0.13283558189868927\n",
      "loss: 0.04876941815018654\n",
      "loss: 0.12728464603424072\n",
      "loss: 0.08074112236499786\n",
      "loss: 0.19780327379703522\n",
      "loss: 0.23300887644290924\n",
      "loss: 0.08212829381227493\n",
      "loss: 0.5062254667282104\n",
      "loss: 0.1963355541229248\n",
      "loss: 0.1011643186211586\n",
      "loss: 0.14553958177566528\n",
      "loss: 0.24524538218975067\n",
      "loss: 0.1360449343919754\n",
      "loss: 0.0786781758069992\n",
      "loss: 0.08844725042581558\n",
      "loss: 0.12756556272506714\n",
      "loss: 0.07397980988025665\n",
      "loss: 0.1407594084739685\n",
      "loss: 0.056835442781448364\n",
      "loss: 0.08706901967525482\n",
      "loss: 0.05604053661227226\n",
      "loss: 0.10397351533174515\n",
      "loss: 0.221430703997612\n",
      "loss: 0.15216636657714844\n",
      "loss: 0.13680991530418396\n",
      "loss: 0.21881833672523499\n",
      "loss: 0.08550330996513367\n",
      "loss: 0.2609793245792389\n",
      "loss: 0.17220334708690643\n",
      "loss: 0.15058813989162445\n",
      "loss: 0.21872827410697937\n",
      "loss: 0.1838400512933731\n",
      "loss: 0.2080439031124115\n",
      "loss: 0.19696667790412903\n",
      "loss: 0.12323945015668869\n",
      "loss: 0.028515838086605072\n",
      "loss: 0.11106691509485245\n",
      "loss: 0.08135299384593964\n",
      "loss: 0.11463358998298645\n",
      "loss: 0.23627372086048126\n",
      "loss: 0.07018198072910309\n",
      "loss: 0.10265184193849564\n",
      "loss: 0.04275871813297272\n",
      "loss: 0.08230967819690704\n",
      "loss: 0.20016445219516754\n",
      "loss: 0.16708792746067047\n",
      "loss: 0.12076335400342941\n",
      "loss: 0.07736963033676147\n",
      "loss: 0.07971269637346268\n",
      "loss: 0.048506032675504684\n",
      "loss: 0.07528674602508545\n",
      "loss: 0.18327254056930542\n",
      "loss: 0.11170672625303268\n",
      "loss: 0.13137440383434296\n",
      "loss: 0.29158106446266174\n",
      "loss: 0.2846309542655945\n",
      "loss: 0.24484393000602722\n",
      "loss: 0.08346743881702423\n",
      "loss: 0.15749187767505646\n",
      "loss: 0.1529819518327713\n",
      "loss: 0.1815149188041687\n",
      "loss: 0.15749435126781464\n",
      "loss: 0.0890267863869667\n",
      "loss: 0.17321592569351196\n",
      "loss: 0.042083002626895905\n",
      "loss: 0.1541936844587326\n",
      "loss: 0.2510647475719452\n",
      "loss: 0.19269393384456635\n",
      "loss: 0.11736823618412018\n",
      "loss: 0.11117780208587646\n",
      "loss: 0.14126597344875336\n",
      "loss: 0.09807609766721725\n",
      "loss: 0.050104640424251556\n",
      "loss: 0.08396202325820923\n",
      "loss: 0.11651371419429779\n",
      "loss: 0.09934724122285843\n",
      "loss: 0.1471400409936905\n",
      "loss: 0.1737177073955536\n",
      "loss: 0.14443524181842804\n",
      "loss: 0.10089149326086044\n",
      "loss: 0.1261325478553772\n",
      "loss: 0.21631170809268951\n",
      "loss: 0.11692462861537933\n",
      "loss: 0.10630971938371658\n",
      "loss: 0.04782987758517265\n",
      "loss: 0.022558920085430145\n",
      "loss: 0.08911417424678802\n",
      "loss: 0.11362924426794052\n",
      "loss: 0.2804527282714844\n",
      "loss: 0.13265828788280487\n",
      "loss: 0.18348978459835052\n",
      "loss: 0.1035689041018486\n",
      "loss: 0.10589559376239777\n",
      "loss: 0.18343129754066467\n",
      "loss: 0.31192895770072937\n",
      "loss: 0.07906133681535721\n",
      "loss: 0.13162733614444733\n",
      "loss: 0.21205508708953857\n",
      "loss: 0.06443901360034943\n",
      "loss: 0.06881145387887955\n",
      "loss: 0.1326705366373062\n",
      "loss: 0.15496273338794708\n",
      "loss: 0.05588384345173836\n",
      "loss: 0.06688925623893738\n",
      "loss: 0.14081238210201263\n",
      "loss: 0.1316576600074768\n",
      "loss: 0.07977932691574097\n",
      "loss: 0.09341960400342941\n",
      "loss: 0.05691339075565338\n",
      "loss: 0.1559201180934906\n",
      "loss: 0.1926400065422058\n",
      "loss: 0.06767762452363968\n",
      "loss: 0.15924757719039917\n",
      "loss: 0.1751503348350525\n",
      "loss: 0.07708991318941116\n",
      "loss: 0.28234201669692993\n",
      "loss: 0.22812536358833313\n",
      "loss: 0.170517697930336\n",
      "loss: 0.13146664202213287\n",
      "loss: 0.1807270050048828\n",
      "loss: 0.11809168756008148\n",
      "loss: 0.16582491993904114\n",
      "loss: 0.2498341202735901\n",
      "loss: 0.23482011258602142\n",
      "loss: 0.12328692525625229\n",
      "loss: 0.23815889656543732\n",
      "loss: 0.17934301495552063\n",
      "loss: 0.07192260026931763\n",
      "loss: 0.13567174971103668\n",
      "loss: 0.1431860774755478\n",
      "loss: 0.2582082152366638\n",
      "loss: 0.25619882345199585\n",
      "loss: 0.12795186042785645\n",
      "loss: 0.1867624670267105\n",
      "loss: 0.09599631279706955\n",
      "loss: 0.12019721418619156\n",
      "loss: 0.2820259928703308\n",
      "loss: 0.09640011191368103\n",
      "loss: 0.14411002397537231\n",
      "loss: 0.11472540348768234\n",
      "loss: 0.08451484888792038\n",
      "loss: 0.28475823998451233\n",
      "loss: 0.06447490304708481\n",
      "loss: 0.0830111876130104\n",
      "loss: 0.0876290425658226\n",
      "loss: 0.26151204109191895\n",
      "loss: 0.14719048142433167\n",
      "loss: 0.06186458095908165\n",
      "loss: 0.3009435832500458\n",
      "loss: 0.10249920934438705\n",
      "loss: 0.0958271399140358\n",
      "loss: 0.19726979732513428\n",
      "loss: 0.09020726382732391\n",
      "loss: 0.1405489444732666\n",
      "loss: 0.14768251776695251\n",
      "loss: 0.17315509915351868\n",
      "loss: 0.3278694748878479\n",
      "loss: 0.19757701456546783\n",
      "loss: 0.09846893697977066\n",
      "loss: 0.16229185461997986\n",
      "loss: 0.038218386471271515\n",
      "loss: 0.04363563656806946\n",
      "loss: 0.1750578135251999\n",
      "loss: 0.047815706580877304\n",
      "loss: 0.25268682837486267\n",
      "loss: 0.13014863431453705\n",
      "loss: 0.08115539699792862\n",
      "loss: 0.08130373060703278\n",
      "loss: 0.2652408480644226\n",
      "loss: 0.0971602275967598\n",
      "loss: 0.0877147987484932\n",
      "loss: 0.06401598453521729\n",
      "loss: 0.11013995856046677\n",
      "loss: 0.07217741012573242\n",
      "loss: 0.24543698132038116\n",
      "loss: 0.08149989694356918\n",
      "loss: 0.08475298434495926\n",
      "loss: 0.13980017602443695\n",
      "loss: 0.06790351122617722\n",
      "loss: 0.18006742000579834\n",
      "loss: 0.331140398979187\n",
      "loss: 0.18819843232631683\n",
      "loss: 0.1458771824836731\n",
      "loss: 0.16368865966796875\n",
      "loss: 0.06748376041650772\n",
      "loss: 0.08401638269424438\n",
      "loss: 0.12364280223846436\n",
      "loss: 0.10153626650571823\n",
      "loss: 0.09635896235704422\n",
      "loss: 0.06788546591997147\n",
      "loss: 0.10671253502368927\n",
      "loss: 0.1734427660703659\n",
      "loss: 0.0977354496717453\n",
      "loss: 0.17270638048648834\n",
      "loss: 0.12834808230400085\n",
      "loss: 0.14286769926548004\n",
      "loss: 0.09835068881511688\n",
      "loss: 0.1858925223350525\n",
      "loss: 0.1170087605714798\n",
      "loss: 0.1714954376220703\n",
      "loss: 0.21096684038639069\n",
      "loss: 0.12276110053062439\n",
      "loss: 0.0400199331343174\n",
      "loss: 0.176136314868927\n",
      "loss: 0.09628069400787354\n",
      "loss: 0.22885215282440186\n",
      "loss: 0.16080601513385773\n",
      "loss: 0.0803358256816864\n",
      "loss: 0.07448292523622513\n",
      "loss: 0.028403444215655327\n",
      "loss: 0.07246478646993637\n",
      "loss: 0.053525373339653015\n",
      "loss: 0.1318981647491455\n",
      "loss: 0.14667347073554993\n",
      "loss: 0.18347473442554474\n",
      "loss: 0.03153720125555992\n",
      "loss: 0.08524700999259949\n",
      "loss: 0.19891169667243958\n",
      "loss: 0.059482697397470474\n",
      "loss: 0.17495182156562805\n",
      "loss: 0.0767766535282135\n",
      "loss: 0.22298678755760193\n",
      "loss: 0.17608626186847687\n",
      "loss: 0.11203023046255112\n",
      "loss: 0.10445792227983475\n",
      "loss: 0.0851486325263977\n",
      "loss: 0.17618520557880402\n",
      "loss: 0.0741039514541626\n",
      "loss: 0.08704684674739838\n",
      "loss: 0.08907701820135117\n",
      "loss: 0.23023410141468048\n",
      "loss: 0.08603521436452866\n",
      "loss: 0.18594026565551758\n",
      "loss: 0.10762186348438263\n",
      "loss: 0.1393851637840271\n",
      "loss: 0.22202883660793304\n",
      "loss: 0.08617047965526581\n",
      "loss: 0.07952842861413956\n",
      "loss: 0.1725202351808548\n",
      "loss: 0.0889832079410553\n",
      "loss: 0.2157430648803711\n",
      "loss: 0.11621459573507309\n",
      "loss: 0.06456504017114639\n",
      "loss: 0.2061215043067932\n",
      "loss: 0.0757034569978714\n",
      "loss: 0.1353835016489029\n",
      "loss: 0.14404265582561493\n",
      "loss: 0.15744079649448395\n",
      "loss: 0.17804257571697235\n",
      "loss: 0.0759376809000969\n",
      "loss: 0.15222643315792084\n",
      "loss: 0.07398299872875214\n",
      "loss: 0.14241421222686768\n",
      "loss: 0.07017838954925537\n",
      "loss: 0.13045237958431244\n",
      "loss: 0.11574738472700119\n",
      "loss: 0.059489548206329346\n",
      "loss: 0.08720267564058304\n",
      "loss: 0.055425699800252914\n",
      "loss: 0.18889515101909637\n",
      "loss: 0.15788130462169647\n",
      "loss: 0.17809748649597168\n",
      "loss: 0.08977466076612473\n",
      "loss: 0.1000019982457161\n",
      "loss: 0.06466454267501831\n",
      "loss: 0.11990910023450851\n",
      "loss: 0.14195962250232697\n",
      "loss: 0.12233715504407883\n",
      "loss: 0.11539523303508759\n",
      "loss: 0.09347338229417801\n",
      "loss: 0.06135597452521324\n",
      "loss: 0.09486746788024902\n",
      "loss: 0.12865445017814636\n",
      "loss: 0.07506335526704788\n",
      "loss: 0.17799687385559082\n",
      "loss: 0.10599517822265625\n",
      "loss: 0.22930248081684113\n",
      "loss: 0.09384901076555252\n",
      "loss: 0.09097243845462799\n",
      "loss: 0.2683250606060028\n",
      "loss: 0.14675553143024445\n",
      "loss: 0.12421385943889618\n",
      "loss: 0.10761704295873642\n",
      "loss: 0.11047451198101044\n",
      "loss: 0.13820776343345642\n",
      "loss: 0.12211760133504868\n",
      "loss: 0.17819657921791077\n",
      "loss: 0.09621164947748184\n",
      "loss: 0.09290599077939987\n",
      "loss: 0.3083121180534363\n",
      "loss: 0.10048134624958038\n",
      "loss: 0.1966366320848465\n",
      "loss: 0.15176980197429657\n",
      "loss: 0.13328199088573456\n",
      "loss: 0.12037979066371918\n",
      "loss: 0.12776997685432434\n",
      "loss: 0.24609079957008362\n",
      "loss: 0.0834474265575409\n",
      "loss: 0.21813184022903442\n",
      "loss: 0.18142689764499664\n",
      "loss: 0.08520929515361786\n",
      "loss: 0.11367077380418777\n",
      "loss: 0.10704295337200165\n",
      "loss: 0.18558374047279358\n",
      "loss: 0.20621192455291748\n",
      "loss: 0.13260306417942047\n",
      "loss: 0.14855141937732697\n",
      "loss: 0.13853515684604645\n",
      "loss: 0.07860647886991501\n",
      "loss: 0.05000290647149086\n",
      "loss: 0.23220375180244446\n",
      "loss: 0.0790271908044815\n",
      "loss: 0.24842596054077148\n",
      "loss: 0.2033088654279709\n",
      "loss: 0.14713193476200104\n",
      "loss: 0.2188342660665512\n",
      "loss: 0.17168505489826202\n",
      "loss: 0.1374427229166031\n",
      "loss: 0.11617743223905563\n",
      "loss: 0.22196777164936066\n",
      "loss: 0.1764589101076126\n",
      "loss: 0.0917341336607933\n",
      "loss: 0.08983402699232101\n",
      "loss: 0.2591659128665924\n",
      "loss: 0.09846168756484985\n",
      "loss: 0.2585647404193878\n",
      "loss: 0.17357973754405975\n",
      "loss: 0.05162116140127182\n",
      "loss: 0.09140026569366455\n",
      "loss: 0.036453861743211746\n",
      "loss: 0.23534739017486572\n",
      "loss: 0.11405496299266815\n",
      "loss: 0.1618652194738388\n",
      "loss: 0.1324734091758728\n",
      "loss: 0.13555501401424408\n",
      "loss: 0.144862100481987\n",
      "loss: 0.2015259712934494\n",
      "loss: 0.09000572562217712\n",
      "loss: 0.21130597591400146\n",
      "loss: 0.10089157521724701\n",
      "loss: 0.1660824418067932\n",
      "loss: 0.08460964262485504\n",
      "loss: 0.07205186784267426\n",
      "loss: 0.14096000790596008\n",
      "loss: 0.1894564926624298\n",
      "loss: 0.20874321460723877\n",
      "loss: 0.07902214676141739\n",
      "loss: 0.2241276055574417\n",
      "loss: 0.07336896657943726\n",
      "loss: 0.19822116196155548\n",
      "loss: 0.11224112659692764\n",
      "loss: 0.16907185316085815\n",
      "loss: 0.11412487179040909\n",
      "loss: 0.153205007314682\n",
      "loss: 0.1073770597577095\n",
      "loss: 0.2864474058151245\n",
      "loss: 0.18062718212604523\n",
      "loss: 0.12196572870016098\n",
      "loss: 0.08140481263399124\n",
      "loss: 0.07075965404510498\n",
      "loss: 0.3374554216861725\n",
      "loss: 0.1321387141942978\n",
      "loss: 0.32676270604133606\n",
      "loss: 0.1308617740869522\n",
      "loss: 0.06255590915679932\n",
      "loss: 0.03903292119503021\n",
      "loss: 0.05523150414228439\n",
      "loss: 0.12121351063251495\n",
      "loss: 0.11417053639888763\n",
      "loss: 0.3297899663448334\n",
      "loss: 0.16379868984222412\n",
      "loss: 0.16240178048610687\n",
      "loss: 0.19585750997066498\n",
      "loss: 0.159743532538414\n",
      "loss: 0.029432933777570724\n",
      "loss: 0.11552520096302032\n",
      "loss: 0.2781725823879242\n",
      "loss: 0.05979197472333908\n",
      "loss: 0.12386331707239151\n",
      "loss: 0.13725391030311584\n",
      "loss: 0.14156459271907806\n",
      "loss: 0.05800783634185791\n",
      "loss: 0.342401385307312\n",
      "loss: 0.14141830801963806\n",
      "loss: 0.3424510955810547\n",
      "loss: 0.19998285174369812\n",
      "loss: 0.17508555948734283\n",
      "loss: 0.1509438306093216\n",
      "loss: 0.0853758156299591\n",
      "loss: 0.06788582354784012\n",
      "loss: 0.13497698307037354\n",
      "loss: 0.18053418397903442\n",
      "loss: 0.11202599108219147\n",
      "loss: 0.15720900893211365\n",
      "loss: 0.17003969848155975\n",
      "loss: 0.13538189232349396\n",
      "loss: 0.20954526960849762\n",
      "loss: 0.08017989248037338\n",
      "loss: 0.18209494650363922\n",
      "loss: 0.13055409491062164\n",
      "loss: 0.15698625147342682\n",
      "loss: 0.13828875124454498\n",
      "loss: 0.0647682249546051\n",
      "loss: 0.06013775244355202\n",
      "loss: 0.15372265875339508\n",
      "loss: 0.23336787521839142\n",
      "loss: 0.09265539795160294\n",
      "loss: 0.16894029080867767\n",
      "loss: 0.08805999159812927\n",
      "loss: 0.1849367916584015\n",
      "loss: 0.04854268580675125\n",
      "loss: 0.14350156486034393\n",
      "loss: 0.11268043518066406\n",
      "loss: 0.12637095153331757\n",
      "loss: 0.0649956464767456\n",
      "loss: 0.1754654049873352\n",
      "loss: 0.20475582778453827\n",
      "loss: 0.06609074026346207\n",
      "loss: 0.07880378514528275\n",
      "loss: 0.23959872126579285\n",
      "loss: 0.19097182154655457\n",
      "loss: 0.13353577256202698\n",
      "loss: 0.21938510239124298\n",
      "loss: 0.12364920973777771\n",
      "loss: 0.08061022311449051\n",
      "loss: 0.1648828089237213\n",
      "loss: 0.2177581489086151\n",
      "loss: 0.08079241961240768\n",
      "loss: 0.11150915920734406\n",
      "loss: 0.08548818528652191\n",
      "loss: 0.13281036913394928\n",
      "loss: 0.06105107069015503\n",
      "loss: 0.2717348337173462\n",
      "loss: 0.06164650246500969\n",
      "loss: 0.05956696346402168\n",
      "loss: 0.09572933614253998\n",
      "loss: 0.06344768404960632\n",
      "loss: 0.09494183957576752\n",
      "loss: 0.07545538246631622\n",
      "loss: 0.23248714208602905\n",
      "loss: 0.09896254539489746\n",
      "loss: 0.05796198919415474\n",
      "loss: 0.13861018419265747\n",
      "loss: 0.1524537205696106\n",
      "loss: 0.15421806275844574\n",
      "loss: 0.19428132474422455\n",
      "loss: 0.07675932347774506\n",
      "loss: 0.0627332255244255\n",
      "loss: 0.1597147434949875\n",
      "loss: 0.15174299478530884\n",
      "loss: 0.12288148701190948\n",
      "loss: 0.15991857647895813\n",
      "loss: 0.08431269228458405\n",
      "loss: 0.2918746769428253\n",
      "loss: 0.12907277047634125\n",
      "loss: 0.18159016966819763\n",
      "loss: 0.12002639472484589\n",
      "loss: 0.04172208160161972\n",
      "loss: 0.0667000412940979\n",
      "loss: 0.11803869158029556\n",
      "loss: 0.0949326902627945\n",
      "loss: 0.15838894248008728\n",
      "loss: 0.13889484107494354\n",
      "loss: 0.13466334342956543\n",
      "loss: 0.18275009095668793\n",
      "loss: 0.197403684258461\n",
      "loss: 0.20362262427806854\n",
      "loss: 0.1459086835384369\n",
      "loss: 0.1316482275724411\n",
      "loss: 0.22119204699993134\n",
      "loss: 0.07012218236923218\n",
      "loss: 0.139372780919075\n",
      "loss: 0.25055110454559326\n",
      "loss: 0.11472561955451965\n",
      "loss: 0.049884721636772156\n",
      "loss: 0.2561819553375244\n",
      "loss: 0.17767731845378876\n",
      "loss: 0.23051103949546814\n",
      "loss: 0.10374613106250763\n",
      "loss: 0.15738974511623383\n",
      "loss: 0.08529885858297348\n",
      "loss: 0.18149195611476898\n",
      "loss: 0.1263870745897293\n",
      "loss: 0.17813341319561005\n",
      "loss: 0.12560109794139862\n",
      "loss: 0.12232115864753723\n",
      "loss: 0.23751382529735565\n",
      "loss: 0.14087314903736115\n",
      "loss: 0.1519811451435089\n",
      "loss: 0.09813523292541504\n",
      "loss: 0.21429334580898285\n",
      "loss: 0.19537603855133057\n",
      "loss: 0.0681731104850769\n",
      "loss: 0.21321804821491241\n",
      "loss: 0.1279190629720688\n",
      "loss: 0.105209119617939\n",
      "loss: 0.1320328563451767\n",
      "loss: 0.342071533203125\n",
      "loss: 0.3075888454914093\n",
      "loss: 0.14131520688533783\n",
      "loss: 0.26522019505500793\n",
      "loss: 0.10263539105653763\n",
      "loss: 0.08227839320898056\n",
      "loss: 0.2719368040561676\n",
      "loss: 0.08337810635566711\n",
      "loss: 0.15711946785449982\n",
      "loss: 0.2455996721982956\n",
      "loss: 0.15050402283668518\n",
      "loss: 0.06458194553852081\n",
      "loss: 0.22908183932304382\n",
      "loss: 0.22891481220722198\n",
      "loss: 0.05325707048177719\n",
      "loss: 0.07174436002969742\n",
      "loss: 0.24598506093025208\n",
      "loss: 0.07781358063220978\n",
      "loss: 0.06095748394727707\n",
      "loss: 0.25525516271591187\n",
      "loss: 0.18240447342395782\n",
      "loss: 0.15469522774219513\n",
      "loss: 0.0770147442817688\n",
      "loss: 0.15467028319835663\n",
      "loss: 0.16250349581241608\n",
      "loss: 0.05217275768518448\n",
      "loss: 0.13071756064891815\n",
      "loss: 0.26244375109672546\n",
      "loss: 0.09321279078722\n",
      "loss: 0.21932819485664368\n",
      "loss: 0.2516951858997345\n",
      "loss: 0.167873814702034\n",
      "loss: 0.08421777188777924\n",
      "loss: 0.10331911593675613\n",
      "loss: 0.0913684070110321\n",
      "loss: 0.1314859241247177\n",
      "loss: 0.22331120073795319\n",
      "loss: 0.14460234344005585\n",
      "loss: 0.05495311692357063\n",
      "loss: 0.13796061277389526\n",
      "loss: 0.055601850152015686\n",
      "loss: 0.2889677882194519\n",
      "loss: 0.17286239564418793\n",
      "loss: 0.07280436903238297\n",
      "loss: 0.08445753902196884\n",
      "loss: 0.17660818994045258\n",
      "loss: 0.15315794944763184\n",
      "loss: 0.32229122519493103\n",
      "loss: 0.04497040808200836\n",
      "loss: 0.07331089675426483\n",
      "loss: 0.1758737564086914\n",
      "loss: 0.10460500419139862\n",
      "loss: 0.2702305316925049\n",
      "loss: 0.09980518370866776\n",
      "loss: 0.09471922367811203\n",
      "loss: 0.16613540053367615\n",
      "loss: 0.19432154297828674\n",
      "loss: 0.24544593691825867\n",
      "loss: 0.07139895111322403\n",
      "loss: 0.22104080021381378\n",
      "loss: 0.08674062043428421\n",
      "loss: 0.14236776530742645\n",
      "loss: 0.10317845642566681\n",
      "loss: 0.12506429851055145\n",
      "loss: 0.14146851003170013\n",
      "loss: 0.26424768567085266\n",
      "loss: 0.03740883991122246\n",
      "loss: 0.073208749294281\n",
      "loss: 0.17346110939979553\n",
      "loss: 0.14750637114048004\n",
      "loss: 0.1132318452000618\n",
      "loss: 0.0670444592833519\n",
      "loss: 0.13260966539382935\n",
      "loss: 0.12546567618846893\n",
      "loss: 0.038252752274274826\n",
      "loss: 0.2073703557252884\n",
      "loss: 0.09752430766820908\n",
      "loss: 0.21277710795402527\n",
      "loss: 0.16961340606212616\n",
      "loss: 0.15920206904411316\n",
      "loss: 0.21949154138565063\n",
      "loss: 0.17692482471466064\n",
      "loss: 0.09107153862714767\n",
      "loss: 0.15666353702545166\n",
      "loss: 0.22547948360443115\n",
      "loss: 0.1656782180070877\n",
      "loss: 0.3145110607147217\n",
      "loss: 0.07116896659135818\n",
      "loss: 0.06679613888263702\n",
      "loss: 0.12099641561508179\n",
      "loss: 0.25567278265953064\n",
      "loss: 0.039792824536561966\n",
      "loss: 0.16175232827663422\n",
      "loss: 0.2169700413942337\n",
      "loss: 0.1461421251296997\n",
      "loss: 0.17982368171215057\n",
      "loss: 0.0823238343000412\n",
      "loss: 0.13868246972560883\n",
      "loss: 0.2667637765407562\n",
      "loss: 0.13292357325553894\n",
      "loss: 0.14161357283592224\n",
      "loss: 0.12593597173690796\n",
      "loss: 0.15337321162223816\n",
      "loss: 0.07817345857620239\n",
      "loss: 0.08005987852811813\n",
      "loss: 0.3210451304912567\n",
      "loss: 0.17081496119499207\n",
      "loss: 0.06796339899301529\n",
      "loss: 0.13219787180423737\n",
      "loss: 0.054869797080755234\n",
      "loss: 0.08029889315366745\n",
      "loss: 0.3386707603931427\n",
      "loss: 0.11259641498327255\n",
      "loss: 0.09457029402256012\n",
      "loss: 0.1600545048713684\n",
      "loss: 0.0977565124630928\n",
      "loss: 0.0395418219268322\n",
      "loss: 0.20172426104545593\n",
      "loss: 0.06374773383140564\n",
      "loss: 0.04329453036189079\n",
      "loss: 0.11254701018333435\n",
      "loss: 0.15943141281604767\n",
      "loss: 0.13005273044109344\n",
      "loss: 0.09525008499622345\n",
      "loss: 0.15389080345630646\n",
      "loss: 0.11156594008207321\n",
      "loss: 0.19457879662513733\n",
      "loss: 0.16771957278251648\n",
      "loss: 0.14747166633605957\n",
      "loss: 0.11193054169416428\n",
      "loss: 0.10249534994363785\n",
      "loss: 0.11970409005880356\n",
      "loss: 0.056556180119514465\n",
      "loss: 0.14982940256595612\n",
      "loss: 0.10448510944843292\n",
      "loss: 0.11961983144283295\n",
      "loss: 0.25414225459098816\n",
      "loss: 0.2559996247291565\n",
      "loss: 0.15391123294830322\n",
      "loss: 0.11004368215799332\n",
      "loss: 0.20719799399375916\n",
      "loss: 0.14790280163288116\n",
      "loss: 0.19036687910556793\n",
      "loss: 0.16684779524803162\n",
      "loss: 0.07297428697347641\n",
      "loss: 0.13008122146129608\n",
      "loss: 0.18225236237049103\n",
      "loss: 0.32102540135383606\n",
      "loss: 0.15987229347229004\n",
      "loss: 0.10170663893222809\n",
      "loss: 0.05510825663805008\n",
      "loss: 0.14816203713417053\n",
      "loss: 0.2174069732427597\n",
      "loss: 0.09764139354228973\n",
      "loss: 0.0901678279042244\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        data = data.reshape(data.shape[0], -1)\n",
    "        scores = model(data)\n",
    "        loss = loss_fn(scores, targets)\n",
    "        \n",
    "        # backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient descent\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f'loss: {loss}')\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data accuracy\n",
      "correct: 62, samples: 64, accuracy: 96.88\n",
      "correct: 125, samples: 128, accuracy: 97.66\n",
      "correct: 188, samples: 192, accuracy: 97.92\n",
      "correct: 249, samples: 256, accuracy: 97.27\n",
      "correct: 310, samples: 320, accuracy: 96.88\n",
      "correct: 372, samples: 384, accuracy: 96.88\n",
      "correct: 435, samples: 448, accuracy: 97.10\n",
      "correct: 498, samples: 512, accuracy: 97.27\n",
      "correct: 559, samples: 576, accuracy: 97.05\n",
      "correct: 617, samples: 640, accuracy: 96.41\n",
      "correct: 676, samples: 704, accuracy: 96.02\n",
      "correct: 739, samples: 768, accuracy: 96.22\n",
      "correct: 800, samples: 832, accuracy: 96.15\n",
      "correct: 863, samples: 896, accuracy: 96.32\n",
      "correct: 925, samples: 960, accuracy: 96.35\n",
      "correct: 985, samples: 1024, accuracy: 96.19\n",
      "correct: 1047, samples: 1088, accuracy: 96.23\n",
      "correct: 1108, samples: 1152, accuracy: 96.18\n",
      "correct: 1168, samples: 1216, accuracy: 96.05\n",
      "correct: 1230, samples: 1280, accuracy: 96.09\n",
      "correct: 1293, samples: 1344, accuracy: 96.21\n",
      "correct: 1356, samples: 1408, accuracy: 96.31\n",
      "correct: 1420, samples: 1472, accuracy: 96.47\n",
      "correct: 1483, samples: 1536, accuracy: 96.55\n",
      "correct: 1546, samples: 1600, accuracy: 96.62\n",
      "correct: 1610, samples: 1664, accuracy: 96.75\n",
      "correct: 1673, samples: 1728, accuracy: 96.82\n",
      "correct: 1736, samples: 1792, accuracy: 96.88\n",
      "correct: 1797, samples: 1856, accuracy: 96.82\n",
      "correct: 1860, samples: 1920, accuracy: 96.88\n",
      "correct: 1922, samples: 1984, accuracy: 96.88\n",
      "correct: 1986, samples: 2048, accuracy: 96.97\n",
      "correct: 2050, samples: 2112, accuracy: 97.06\n",
      "correct: 2109, samples: 2176, accuracy: 96.92\n",
      "correct: 2171, samples: 2240, accuracy: 96.92\n",
      "correct: 2233, samples: 2304, accuracy: 96.92\n",
      "correct: 2296, samples: 2368, accuracy: 96.96\n",
      "correct: 2357, samples: 2432, accuracy: 96.92\n",
      "correct: 2418, samples: 2496, accuracy: 96.88\n",
      "correct: 2482, samples: 2560, accuracy: 96.95\n",
      "correct: 2546, samples: 2624, accuracy: 97.03\n",
      "correct: 2609, samples: 2688, accuracy: 97.06\n",
      "correct: 2671, samples: 2752, accuracy: 97.06\n",
      "correct: 2731, samples: 2816, accuracy: 96.98\n",
      "correct: 2792, samples: 2880, accuracy: 96.94\n",
      "correct: 2853, samples: 2944, accuracy: 96.91\n",
      "correct: 2914, samples: 3008, accuracy: 96.88\n",
      "correct: 2976, samples: 3072, accuracy: 96.88\n",
      "correct: 3036, samples: 3136, accuracy: 96.81\n",
      "correct: 3096, samples: 3200, accuracy: 96.75\n",
      "correct: 3157, samples: 3264, accuracy: 96.72\n",
      "correct: 3220, samples: 3328, accuracy: 96.75\n",
      "correct: 3282, samples: 3392, accuracy: 96.76\n",
      "correct: 3342, samples: 3456, accuracy: 96.70\n",
      "correct: 3402, samples: 3520, accuracy: 96.65\n",
      "correct: 3466, samples: 3584, accuracy: 96.71\n",
      "correct: 3528, samples: 3648, accuracy: 96.71\n",
      "correct: 3591, samples: 3712, accuracy: 96.74\n",
      "correct: 3651, samples: 3776, accuracy: 96.69\n",
      "correct: 3711, samples: 3840, accuracy: 96.64\n",
      "correct: 3771, samples: 3904, accuracy: 96.59\n",
      "correct: 3835, samples: 3968, accuracy: 96.65\n",
      "correct: 3895, samples: 4032, accuracy: 96.60\n",
      "correct: 3956, samples: 4096, accuracy: 96.58\n",
      "correct: 4018, samples: 4160, accuracy: 96.59\n",
      "correct: 4081, samples: 4224, accuracy: 96.61\n",
      "correct: 4144, samples: 4288, accuracy: 96.64\n",
      "correct: 4206, samples: 4352, accuracy: 96.65\n",
      "correct: 4269, samples: 4416, accuracy: 96.67\n",
      "correct: 4330, samples: 4480, accuracy: 96.65\n",
      "correct: 4389, samples: 4544, accuracy: 96.59\n",
      "correct: 4453, samples: 4608, accuracy: 96.64\n",
      "correct: 4515, samples: 4672, accuracy: 96.64\n",
      "correct: 4576, samples: 4736, accuracy: 96.62\n",
      "correct: 4637, samples: 4800, accuracy: 96.60\n",
      "correct: 4699, samples: 4864, accuracy: 96.61\n",
      "correct: 4758, samples: 4928, accuracy: 96.55\n",
      "correct: 4821, samples: 4992, accuracy: 96.57\n",
      "correct: 4878, samples: 5056, accuracy: 96.48\n",
      "correct: 4942, samples: 5120, accuracy: 96.52\n",
      "correct: 5003, samples: 5184, accuracy: 96.51\n",
      "correct: 5064, samples: 5248, accuracy: 96.49\n",
      "correct: 5127, samples: 5312, accuracy: 96.52\n",
      "correct: 5189, samples: 5376, accuracy: 96.52\n",
      "correct: 5252, samples: 5440, accuracy: 96.54\n",
      "correct: 5316, samples: 5504, accuracy: 96.58\n",
      "correct: 5378, samples: 5568, accuracy: 96.59\n",
      "correct: 5441, samples: 5632, accuracy: 96.61\n",
      "correct: 5504, samples: 5696, accuracy: 96.63\n",
      "correct: 5564, samples: 5760, accuracy: 96.60\n",
      "correct: 5625, samples: 5824, accuracy: 96.58\n",
      "correct: 5689, samples: 5888, accuracy: 96.62\n",
      "correct: 5753, samples: 5952, accuracy: 96.66\n",
      "correct: 5815, samples: 6016, accuracy: 96.66\n",
      "correct: 5878, samples: 6080, accuracy: 96.68\n",
      "correct: 5938, samples: 6144, accuracy: 96.65\n",
      "correct: 5998, samples: 6208, accuracy: 96.62\n",
      "correct: 6062, samples: 6272, accuracy: 96.65\n",
      "correct: 6121, samples: 6336, accuracy: 96.61\n",
      "correct: 6181, samples: 6400, accuracy: 96.58\n",
      "correct: 6242, samples: 6464, accuracy: 96.57\n",
      "correct: 6306, samples: 6528, accuracy: 96.60\n",
      "correct: 6369, samples: 6592, accuracy: 96.62\n",
      "correct: 6431, samples: 6656, accuracy: 96.62\n",
      "correct: 6495, samples: 6720, accuracy: 96.65\n",
      "correct: 6555, samples: 6784, accuracy: 96.62\n",
      "correct: 6618, samples: 6848, accuracy: 96.64\n",
      "correct: 6681, samples: 6912, accuracy: 96.66\n",
      "correct: 6745, samples: 6976, accuracy: 96.69\n",
      "correct: 6806, samples: 7040, accuracy: 96.68\n",
      "correct: 6868, samples: 7104, accuracy: 96.68\n",
      "correct: 6930, samples: 7168, accuracy: 96.68\n",
      "correct: 6994, samples: 7232, accuracy: 96.71\n",
      "correct: 7057, samples: 7296, accuracy: 96.72\n",
      "correct: 7119, samples: 7360, accuracy: 96.73\n",
      "correct: 7181, samples: 7424, accuracy: 96.73\n",
      "correct: 7238, samples: 7488, accuracy: 96.66\n",
      "correct: 7301, samples: 7552, accuracy: 96.68\n",
      "correct: 7363, samples: 7616, accuracy: 96.68\n",
      "correct: 7425, samples: 7680, accuracy: 96.68\n",
      "correct: 7487, samples: 7744, accuracy: 96.68\n",
      "correct: 7547, samples: 7808, accuracy: 96.66\n",
      "correct: 7608, samples: 7872, accuracy: 96.65\n",
      "correct: 7671, samples: 7936, accuracy: 96.66\n",
      "correct: 7733, samples: 8000, accuracy: 96.66\n",
      "correct: 7796, samples: 8064, accuracy: 96.68\n",
      "correct: 7859, samples: 8128, accuracy: 96.69\n",
      "correct: 7922, samples: 8192, accuracy: 96.70\n",
      "correct: 7983, samples: 8256, accuracy: 96.69\n",
      "correct: 8045, samples: 8320, accuracy: 96.69\n",
      "correct: 8107, samples: 8384, accuracy: 96.70\n",
      "correct: 8169, samples: 8448, accuracy: 96.70\n",
      "correct: 8230, samples: 8512, accuracy: 96.69\n",
      "correct: 8291, samples: 8576, accuracy: 96.68\n",
      "correct: 8354, samples: 8640, accuracy: 96.69\n",
      "correct: 8417, samples: 8704, accuracy: 96.70\n",
      "correct: 8480, samples: 8768, accuracy: 96.72\n",
      "correct: 8543, samples: 8832, accuracy: 96.73\n",
      "correct: 8604, samples: 8896, accuracy: 96.72\n",
      "correct: 8663, samples: 8960, accuracy: 96.69\n",
      "correct: 8727, samples: 9024, accuracy: 96.71\n",
      "correct: 8789, samples: 9088, accuracy: 96.71\n",
      "correct: 8852, samples: 9152, accuracy: 96.72\n",
      "correct: 8914, samples: 9216, accuracy: 96.72\n",
      "correct: 8976, samples: 9280, accuracy: 96.72\n",
      "correct: 9037, samples: 9344, accuracy: 96.71\n",
      "correct: 9100, samples: 9408, accuracy: 96.73\n",
      "correct: 9161, samples: 9472, accuracy: 96.72\n",
      "correct: 9224, samples: 9536, accuracy: 96.73\n",
      "correct: 9284, samples: 9600, accuracy: 96.71\n",
      "correct: 9345, samples: 9664, accuracy: 96.70\n",
      "correct: 9405, samples: 9728, accuracy: 96.68\n",
      "correct: 9465, samples: 9792, accuracy: 96.66\n",
      "correct: 9525, samples: 9856, accuracy: 96.64\n",
      "correct: 9589, samples: 9920, accuracy: 96.66\n",
      "correct: 9650, samples: 9984, accuracy: 96.65\n",
      "correct: 9713, samples: 10048, accuracy: 96.67\n",
      "correct: 9775, samples: 10112, accuracy: 96.67\n",
      "correct: 9838, samples: 10176, accuracy: 96.68\n",
      "correct: 9900, samples: 10240, accuracy: 96.68\n",
      "correct: 9962, samples: 10304, accuracy: 96.68\n",
      "correct: 10023, samples: 10368, accuracy: 96.67\n",
      "correct: 10087, samples: 10432, accuracy: 96.69\n",
      "correct: 10148, samples: 10496, accuracy: 96.68\n",
      "correct: 10210, samples: 10560, accuracy: 96.69\n",
      "correct: 10269, samples: 10624, accuracy: 96.66\n",
      "correct: 10330, samples: 10688, accuracy: 96.65\n",
      "correct: 10392, samples: 10752, accuracy: 96.65\n",
      "correct: 10454, samples: 10816, accuracy: 96.65\n",
      "correct: 10514, samples: 10880, accuracy: 96.64\n",
      "correct: 10576, samples: 10944, accuracy: 96.64\n",
      "correct: 10639, samples: 11008, accuracy: 96.65\n",
      "correct: 10702, samples: 11072, accuracy: 96.66\n",
      "correct: 10763, samples: 11136, accuracy: 96.65\n",
      "correct: 10826, samples: 11200, accuracy: 96.66\n",
      "correct: 10889, samples: 11264, accuracy: 96.67\n",
      "correct: 10950, samples: 11328, accuracy: 96.66\n",
      "correct: 11013, samples: 11392, accuracy: 96.67\n",
      "correct: 11075, samples: 11456, accuracy: 96.67\n",
      "correct: 11136, samples: 11520, accuracy: 96.67\n",
      "correct: 11199, samples: 11584, accuracy: 96.68\n",
      "correct: 11260, samples: 11648, accuracy: 96.67\n",
      "correct: 11323, samples: 11712, accuracy: 96.68\n",
      "correct: 11386, samples: 11776, accuracy: 96.69\n",
      "correct: 11448, samples: 11840, accuracy: 96.69\n",
      "correct: 11507, samples: 11904, accuracy: 96.66\n",
      "correct: 11568, samples: 11968, accuracy: 96.66\n",
      "correct: 11628, samples: 12032, accuracy: 96.64\n",
      "correct: 11689, samples: 12096, accuracy: 96.64\n",
      "correct: 11750, samples: 12160, accuracy: 96.63\n",
      "correct: 11809, samples: 12224, accuracy: 96.61\n",
      "correct: 11869, samples: 12288, accuracy: 96.59\n",
      "correct: 11933, samples: 12352, accuracy: 96.61\n",
      "correct: 11996, samples: 12416, accuracy: 96.62\n",
      "correct: 12056, samples: 12480, accuracy: 96.60\n",
      "correct: 12119, samples: 12544, accuracy: 96.61\n",
      "correct: 12179, samples: 12608, accuracy: 96.60\n",
      "correct: 12241, samples: 12672, accuracy: 96.60\n",
      "correct: 12304, samples: 12736, accuracy: 96.61\n",
      "correct: 12366, samples: 12800, accuracy: 96.61\n",
      "correct: 12427, samples: 12864, accuracy: 96.60\n",
      "correct: 12488, samples: 12928, accuracy: 96.60\n",
      "correct: 12547, samples: 12992, accuracy: 96.57\n",
      "correct: 12607, samples: 13056, accuracy: 96.56\n",
      "correct: 12667, samples: 13120, accuracy: 96.55\n",
      "correct: 12728, samples: 13184, accuracy: 96.54\n",
      "correct: 12788, samples: 13248, accuracy: 96.53\n",
      "correct: 12851, samples: 13312, accuracy: 96.54\n",
      "correct: 12913, samples: 13376, accuracy: 96.54\n",
      "correct: 12974, samples: 13440, accuracy: 96.53\n",
      "correct: 13036, samples: 13504, accuracy: 96.53\n",
      "correct: 13100, samples: 13568, accuracy: 96.55\n",
      "correct: 13162, samples: 13632, accuracy: 96.55\n",
      "correct: 13225, samples: 13696, accuracy: 96.56\n",
      "correct: 13289, samples: 13760, accuracy: 96.58\n",
      "correct: 13352, samples: 13824, accuracy: 96.59\n",
      "correct: 13413, samples: 13888, accuracy: 96.58\n",
      "correct: 13477, samples: 13952, accuracy: 96.60\n",
      "correct: 13540, samples: 14016, accuracy: 96.60\n",
      "correct: 13598, samples: 14080, accuracy: 96.58\n",
      "correct: 13659, samples: 14144, accuracy: 96.57\n",
      "correct: 13722, samples: 14208, accuracy: 96.58\n",
      "correct: 13786, samples: 14272, accuracy: 96.59\n",
      "correct: 13848, samples: 14336, accuracy: 96.60\n",
      "correct: 13909, samples: 14400, accuracy: 96.59\n",
      "correct: 13970, samples: 14464, accuracy: 96.58\n",
      "correct: 14031, samples: 14528, accuracy: 96.58\n",
      "correct: 14091, samples: 14592, accuracy: 96.57\n",
      "correct: 14152, samples: 14656, accuracy: 96.56\n",
      "correct: 14213, samples: 14720, accuracy: 96.56\n",
      "correct: 14276, samples: 14784, accuracy: 96.56\n",
      "correct: 14340, samples: 14848, accuracy: 96.58\n",
      "correct: 14400, samples: 14912, accuracy: 96.57\n",
      "correct: 14462, samples: 14976, accuracy: 96.57\n",
      "correct: 14525, samples: 15040, accuracy: 96.58\n",
      "correct: 14588, samples: 15104, accuracy: 96.58\n",
      "correct: 14649, samples: 15168, accuracy: 96.58\n",
      "correct: 14711, samples: 15232, accuracy: 96.58\n",
      "correct: 14774, samples: 15296, accuracy: 96.59\n",
      "correct: 14835, samples: 15360, accuracy: 96.58\n",
      "correct: 14899, samples: 15424, accuracy: 96.60\n",
      "correct: 14961, samples: 15488, accuracy: 96.60\n",
      "correct: 15023, samples: 15552, accuracy: 96.60\n",
      "correct: 15085, samples: 15616, accuracy: 96.60\n",
      "correct: 15145, samples: 15680, accuracy: 96.59\n",
      "correct: 15206, samples: 15744, accuracy: 96.58\n",
      "correct: 15270, samples: 15808, accuracy: 96.60\n",
      "correct: 15332, samples: 15872, accuracy: 96.60\n",
      "correct: 15394, samples: 15936, accuracy: 96.60\n",
      "correct: 15455, samples: 16000, accuracy: 96.59\n",
      "correct: 15515, samples: 16064, accuracy: 96.58\n",
      "correct: 15577, samples: 16128, accuracy: 96.58\n",
      "correct: 15638, samples: 16192, accuracy: 96.58\n",
      "correct: 15699, samples: 16256, accuracy: 96.57\n",
      "correct: 15762, samples: 16320, accuracy: 96.58\n",
      "correct: 15826, samples: 16384, accuracy: 96.59\n",
      "correct: 15889, samples: 16448, accuracy: 96.60\n",
      "correct: 15949, samples: 16512, accuracy: 96.59\n",
      "correct: 16009, samples: 16576, accuracy: 96.58\n",
      "correct: 16071, samples: 16640, accuracy: 96.58\n",
      "correct: 16133, samples: 16704, accuracy: 96.58\n",
      "correct: 16197, samples: 16768, accuracy: 96.59\n",
      "correct: 16258, samples: 16832, accuracy: 96.59\n",
      "correct: 16319, samples: 16896, accuracy: 96.58\n",
      "correct: 16378, samples: 16960, accuracy: 96.57\n",
      "correct: 16439, samples: 17024, accuracy: 96.56\n",
      "correct: 16500, samples: 17088, accuracy: 96.56\n",
      "correct: 16558, samples: 17152, accuracy: 96.54\n",
      "correct: 16619, samples: 17216, accuracy: 96.53\n",
      "correct: 16680, samples: 17280, accuracy: 96.53\n",
      "correct: 16743, samples: 17344, accuracy: 96.53\n",
      "correct: 16806, samples: 17408, accuracy: 96.54\n",
      "correct: 16869, samples: 17472, accuracy: 96.55\n",
      "correct: 16930, samples: 17536, accuracy: 96.54\n",
      "correct: 16990, samples: 17600, accuracy: 96.53\n",
      "correct: 17053, samples: 17664, accuracy: 96.54\n",
      "correct: 17115, samples: 17728, accuracy: 96.54\n",
      "correct: 17174, samples: 17792, accuracy: 96.53\n",
      "correct: 17236, samples: 17856, accuracy: 96.53\n",
      "correct: 17300, samples: 17920, accuracy: 96.54\n",
      "correct: 17362, samples: 17984, accuracy: 96.54\n",
      "correct: 17425, samples: 18048, accuracy: 96.55\n",
      "correct: 17488, samples: 18112, accuracy: 96.55\n",
      "correct: 17550, samples: 18176, accuracy: 96.56\n",
      "correct: 17610, samples: 18240, accuracy: 96.55\n",
      "correct: 17673, samples: 18304, accuracy: 96.55\n",
      "correct: 17732, samples: 18368, accuracy: 96.54\n",
      "correct: 17794, samples: 18432, accuracy: 96.54\n",
      "correct: 17857, samples: 18496, accuracy: 96.55\n",
      "correct: 17918, samples: 18560, accuracy: 96.54\n",
      "correct: 17980, samples: 18624, accuracy: 96.54\n",
      "correct: 18043, samples: 18688, accuracy: 96.55\n",
      "correct: 18105, samples: 18752, accuracy: 96.55\n",
      "correct: 18166, samples: 18816, accuracy: 96.55\n",
      "correct: 18229, samples: 18880, accuracy: 96.55\n",
      "correct: 18289, samples: 18944, accuracy: 96.54\n",
      "correct: 18351, samples: 19008, accuracy: 96.54\n",
      "correct: 18411, samples: 19072, accuracy: 96.53\n",
      "correct: 18472, samples: 19136, accuracy: 96.53\n",
      "correct: 18536, samples: 19200, accuracy: 96.54\n",
      "correct: 18600, samples: 19264, accuracy: 96.55\n",
      "correct: 18662, samples: 19328, accuracy: 96.55\n",
      "correct: 18724, samples: 19392, accuracy: 96.56\n",
      "correct: 18788, samples: 19456, accuracy: 96.57\n",
      "correct: 18852, samples: 19520, accuracy: 96.58\n",
      "correct: 18916, samples: 19584, accuracy: 96.59\n",
      "correct: 18978, samples: 19648, accuracy: 96.59\n",
      "correct: 19039, samples: 19712, accuracy: 96.59\n",
      "correct: 19100, samples: 19776, accuracy: 96.58\n",
      "correct: 19162, samples: 19840, accuracy: 96.58\n",
      "correct: 19222, samples: 19904, accuracy: 96.57\n",
      "correct: 19284, samples: 19968, accuracy: 96.57\n",
      "correct: 19347, samples: 20032, accuracy: 96.58\n",
      "correct: 19407, samples: 20096, accuracy: 96.57\n",
      "correct: 19471, samples: 20160, accuracy: 96.58\n",
      "correct: 19533, samples: 20224, accuracy: 96.58\n",
      "correct: 19596, samples: 20288, accuracy: 96.59\n",
      "correct: 19657, samples: 20352, accuracy: 96.59\n",
      "correct: 19717, samples: 20416, accuracy: 96.58\n",
      "correct: 19779, samples: 20480, accuracy: 96.58\n",
      "correct: 19840, samples: 20544, accuracy: 96.57\n",
      "correct: 19903, samples: 20608, accuracy: 96.58\n",
      "correct: 19966, samples: 20672, accuracy: 96.58\n",
      "correct: 20028, samples: 20736, accuracy: 96.59\n",
      "correct: 20089, samples: 20800, accuracy: 96.58\n",
      "correct: 20151, samples: 20864, accuracy: 96.58\n",
      "correct: 20213, samples: 20928, accuracy: 96.58\n",
      "correct: 20275, samples: 20992, accuracy: 96.58\n",
      "correct: 20338, samples: 21056, accuracy: 96.59\n",
      "correct: 20399, samples: 21120, accuracy: 96.59\n",
      "correct: 20459, samples: 21184, accuracy: 96.58\n",
      "correct: 20520, samples: 21248, accuracy: 96.57\n",
      "correct: 20583, samples: 21312, accuracy: 96.58\n",
      "correct: 20642, samples: 21376, accuracy: 96.57\n",
      "correct: 20705, samples: 21440, accuracy: 96.57\n",
      "correct: 20768, samples: 21504, accuracy: 96.58\n",
      "correct: 20830, samples: 21568, accuracy: 96.58\n",
      "correct: 20894, samples: 21632, accuracy: 96.59\n",
      "correct: 20958, samples: 21696, accuracy: 96.60\n",
      "correct: 21020, samples: 21760, accuracy: 96.60\n",
      "correct: 21083, samples: 21824, accuracy: 96.60\n",
      "correct: 21146, samples: 21888, accuracy: 96.61\n",
      "correct: 21206, samples: 21952, accuracy: 96.60\n",
      "correct: 21268, samples: 22016, accuracy: 96.60\n",
      "correct: 21332, samples: 22080, accuracy: 96.61\n",
      "correct: 21394, samples: 22144, accuracy: 96.61\n",
      "correct: 21454, samples: 22208, accuracy: 96.60\n",
      "correct: 21517, samples: 22272, accuracy: 96.61\n",
      "correct: 21579, samples: 22336, accuracy: 96.61\n",
      "correct: 21641, samples: 22400, accuracy: 96.61\n",
      "correct: 21702, samples: 22464, accuracy: 96.61\n",
      "correct: 21763, samples: 22528, accuracy: 96.60\n",
      "correct: 21825, samples: 22592, accuracy: 96.60\n",
      "correct: 21886, samples: 22656, accuracy: 96.60\n",
      "correct: 21948, samples: 22720, accuracy: 96.60\n",
      "correct: 22011, samples: 22784, accuracy: 96.61\n",
      "correct: 22074, samples: 22848, accuracy: 96.61\n",
      "correct: 22138, samples: 22912, accuracy: 96.62\n",
      "correct: 22201, samples: 22976, accuracy: 96.63\n",
      "correct: 22264, samples: 23040, accuracy: 96.63\n",
      "correct: 22325, samples: 23104, accuracy: 96.63\n",
      "correct: 22388, samples: 23168, accuracy: 96.63\n",
      "correct: 22450, samples: 23232, accuracy: 96.63\n",
      "correct: 22513, samples: 23296, accuracy: 96.64\n",
      "correct: 22575, samples: 23360, accuracy: 96.64\n",
      "correct: 22639, samples: 23424, accuracy: 96.65\n",
      "correct: 22701, samples: 23488, accuracy: 96.65\n",
      "correct: 22763, samples: 23552, accuracy: 96.65\n",
      "correct: 22825, samples: 23616, accuracy: 96.65\n",
      "correct: 22888, samples: 23680, accuracy: 96.66\n",
      "correct: 22945, samples: 23744, accuracy: 96.63\n",
      "correct: 23007, samples: 23808, accuracy: 96.64\n",
      "correct: 23068, samples: 23872, accuracy: 96.63\n",
      "correct: 23128, samples: 23936, accuracy: 96.62\n",
      "correct: 23189, samples: 24000, accuracy: 96.62\n",
      "correct: 23251, samples: 24064, accuracy: 96.62\n",
      "correct: 23314, samples: 24128, accuracy: 96.63\n",
      "correct: 23375, samples: 24192, accuracy: 96.62\n",
      "correct: 23437, samples: 24256, accuracy: 96.62\n",
      "correct: 23498, samples: 24320, accuracy: 96.62\n",
      "correct: 23561, samples: 24384, accuracy: 96.62\n",
      "correct: 23625, samples: 24448, accuracy: 96.63\n",
      "correct: 23688, samples: 24512, accuracy: 96.64\n",
      "correct: 23747, samples: 24576, accuracy: 96.63\n",
      "correct: 23808, samples: 24640, accuracy: 96.62\n",
      "correct: 23870, samples: 24704, accuracy: 96.62\n",
      "correct: 23927, samples: 24768, accuracy: 96.60\n",
      "correct: 23989, samples: 24832, accuracy: 96.61\n",
      "correct: 24053, samples: 24896, accuracy: 96.61\n",
      "correct: 24115, samples: 24960, accuracy: 96.61\n",
      "correct: 24177, samples: 25024, accuracy: 96.62\n",
      "correct: 24241, samples: 25088, accuracy: 96.62\n",
      "correct: 24304, samples: 25152, accuracy: 96.63\n",
      "correct: 24364, samples: 25216, accuracy: 96.62\n",
      "correct: 24428, samples: 25280, accuracy: 96.63\n",
      "correct: 24490, samples: 25344, accuracy: 96.63\n",
      "correct: 24552, samples: 25408, accuracy: 96.63\n",
      "correct: 24615, samples: 25472, accuracy: 96.64\n",
      "correct: 24676, samples: 25536, accuracy: 96.63\n",
      "correct: 24738, samples: 25600, accuracy: 96.63\n",
      "correct: 24798, samples: 25664, accuracy: 96.63\n",
      "correct: 24859, samples: 25728, accuracy: 96.62\n",
      "correct: 24922, samples: 25792, accuracy: 96.63\n",
      "correct: 24984, samples: 25856, accuracy: 96.63\n",
      "correct: 25048, samples: 25920, accuracy: 96.64\n",
      "correct: 25110, samples: 25984, accuracy: 96.64\n",
      "correct: 25172, samples: 26048, accuracy: 96.64\n",
      "correct: 25233, samples: 26112, accuracy: 96.63\n",
      "correct: 25295, samples: 26176, accuracy: 96.63\n",
      "correct: 25356, samples: 26240, accuracy: 96.63\n",
      "correct: 25415, samples: 26304, accuracy: 96.62\n",
      "correct: 25474, samples: 26368, accuracy: 96.61\n",
      "correct: 25535, samples: 26432, accuracy: 96.61\n",
      "correct: 25595, samples: 26496, accuracy: 96.60\n",
      "correct: 25658, samples: 26560, accuracy: 96.60\n",
      "correct: 25721, samples: 26624, accuracy: 96.61\n",
      "correct: 25783, samples: 26688, accuracy: 96.61\n",
      "correct: 25844, samples: 26752, accuracy: 96.61\n",
      "correct: 25905, samples: 26816, accuracy: 96.60\n",
      "correct: 25966, samples: 26880, accuracy: 96.60\n",
      "correct: 26027, samples: 26944, accuracy: 96.60\n",
      "correct: 26087, samples: 27008, accuracy: 96.59\n",
      "correct: 26148, samples: 27072, accuracy: 96.59\n",
      "correct: 26212, samples: 27136, accuracy: 96.59\n",
      "correct: 26274, samples: 27200, accuracy: 96.60\n",
      "correct: 26338, samples: 27264, accuracy: 96.60\n",
      "correct: 26400, samples: 27328, accuracy: 96.60\n",
      "correct: 26463, samples: 27392, accuracy: 96.61\n",
      "correct: 26524, samples: 27456, accuracy: 96.61\n",
      "correct: 26583, samples: 27520, accuracy: 96.60\n",
      "correct: 26647, samples: 27584, accuracy: 96.60\n",
      "correct: 26709, samples: 27648, accuracy: 96.60\n",
      "correct: 26772, samples: 27712, accuracy: 96.61\n",
      "correct: 26833, samples: 27776, accuracy: 96.60\n",
      "correct: 26896, samples: 27840, accuracy: 96.61\n",
      "correct: 26955, samples: 27904, accuracy: 96.60\n",
      "correct: 27015, samples: 27968, accuracy: 96.59\n",
      "correct: 27078, samples: 28032, accuracy: 96.60\n",
      "correct: 27137, samples: 28096, accuracy: 96.59\n",
      "correct: 27199, samples: 28160, accuracy: 96.59\n",
      "correct: 27260, samples: 28224, accuracy: 96.58\n",
      "correct: 27324, samples: 28288, accuracy: 96.59\n",
      "correct: 27387, samples: 28352, accuracy: 96.60\n",
      "correct: 27449, samples: 28416, accuracy: 96.60\n",
      "correct: 27510, samples: 28480, accuracy: 96.59\n",
      "correct: 27572, samples: 28544, accuracy: 96.59\n",
      "correct: 27632, samples: 28608, accuracy: 96.59\n",
      "correct: 27695, samples: 28672, accuracy: 96.59\n",
      "correct: 27757, samples: 28736, accuracy: 96.59\n",
      "correct: 27819, samples: 28800, accuracy: 96.59\n",
      "correct: 27879, samples: 28864, accuracy: 96.59\n",
      "correct: 27941, samples: 28928, accuracy: 96.59\n",
      "correct: 28004, samples: 28992, accuracy: 96.59\n",
      "correct: 28066, samples: 29056, accuracy: 96.59\n",
      "correct: 28130, samples: 29120, accuracy: 96.60\n",
      "correct: 28192, samples: 29184, accuracy: 96.60\n",
      "correct: 28254, samples: 29248, accuracy: 96.60\n",
      "correct: 28312, samples: 29312, accuracy: 96.59\n",
      "correct: 28373, samples: 29376, accuracy: 96.59\n",
      "correct: 28434, samples: 29440, accuracy: 96.58\n",
      "correct: 28496, samples: 29504, accuracy: 96.58\n",
      "correct: 28559, samples: 29568, accuracy: 96.59\n",
      "correct: 28623, samples: 29632, accuracy: 96.59\n",
      "correct: 28684, samples: 29696, accuracy: 96.59\n",
      "correct: 28745, samples: 29760, accuracy: 96.59\n",
      "correct: 28807, samples: 29824, accuracy: 96.59\n",
      "correct: 28870, samples: 29888, accuracy: 96.59\n",
      "correct: 28934, samples: 29952, accuracy: 96.60\n",
      "correct: 28996, samples: 30016, accuracy: 96.60\n",
      "correct: 29058, samples: 30080, accuracy: 96.60\n",
      "correct: 29120, samples: 30144, accuracy: 96.60\n",
      "correct: 29181, samples: 30208, accuracy: 96.60\n",
      "correct: 29244, samples: 30272, accuracy: 96.60\n",
      "correct: 29307, samples: 30336, accuracy: 96.61\n",
      "correct: 29368, samples: 30400, accuracy: 96.61\n",
      "correct: 29430, samples: 30464, accuracy: 96.61\n",
      "correct: 29491, samples: 30528, accuracy: 96.60\n",
      "correct: 29553, samples: 30592, accuracy: 96.60\n",
      "correct: 29616, samples: 30656, accuracy: 96.61\n",
      "correct: 29679, samples: 30720, accuracy: 96.61\n",
      "correct: 29740, samples: 30784, accuracy: 96.61\n",
      "correct: 29801, samples: 30848, accuracy: 96.61\n",
      "correct: 29864, samples: 30912, accuracy: 96.61\n",
      "correct: 29928, samples: 30976, accuracy: 96.62\n",
      "correct: 29991, samples: 31040, accuracy: 96.62\n",
      "correct: 30053, samples: 31104, accuracy: 96.62\n",
      "correct: 30116, samples: 31168, accuracy: 96.62\n",
      "correct: 30178, samples: 31232, accuracy: 96.63\n",
      "correct: 30236, samples: 31296, accuracy: 96.61\n",
      "correct: 30296, samples: 31360, accuracy: 96.61\n",
      "correct: 30359, samples: 31424, accuracy: 96.61\n",
      "correct: 30422, samples: 31488, accuracy: 96.61\n",
      "correct: 30484, samples: 31552, accuracy: 96.62\n",
      "correct: 30543, samples: 31616, accuracy: 96.61\n",
      "correct: 30607, samples: 31680, accuracy: 96.61\n",
      "correct: 30668, samples: 31744, accuracy: 96.61\n",
      "correct: 30730, samples: 31808, accuracy: 96.61\n",
      "correct: 30792, samples: 31872, accuracy: 96.61\n",
      "correct: 30856, samples: 31936, accuracy: 96.62\n",
      "correct: 30917, samples: 32000, accuracy: 96.62\n",
      "correct: 30979, samples: 32064, accuracy: 96.62\n",
      "correct: 31040, samples: 32128, accuracy: 96.61\n",
      "correct: 31102, samples: 32192, accuracy: 96.61\n",
      "correct: 31163, samples: 32256, accuracy: 96.61\n",
      "correct: 31224, samples: 32320, accuracy: 96.61\n",
      "correct: 31288, samples: 32384, accuracy: 96.62\n",
      "correct: 31352, samples: 32448, accuracy: 96.62\n",
      "correct: 31413, samples: 32512, accuracy: 96.62\n",
      "correct: 31472, samples: 32576, accuracy: 96.61\n",
      "correct: 31533, samples: 32640, accuracy: 96.61\n",
      "correct: 31594, samples: 32704, accuracy: 96.61\n",
      "correct: 31656, samples: 32768, accuracy: 96.61\n",
      "correct: 31718, samples: 32832, accuracy: 96.61\n",
      "correct: 31780, samples: 32896, accuracy: 96.61\n",
      "correct: 31843, samples: 32960, accuracy: 96.61\n",
      "correct: 31904, samples: 33024, accuracy: 96.61\n",
      "correct: 31965, samples: 33088, accuracy: 96.61\n",
      "correct: 32028, samples: 33152, accuracy: 96.61\n",
      "correct: 32090, samples: 33216, accuracy: 96.61\n",
      "correct: 32152, samples: 33280, accuracy: 96.61\n",
      "correct: 32215, samples: 33344, accuracy: 96.61\n",
      "correct: 32277, samples: 33408, accuracy: 96.61\n",
      "correct: 32337, samples: 33472, accuracy: 96.61\n",
      "correct: 32400, samples: 33536, accuracy: 96.61\n",
      "correct: 32462, samples: 33600, accuracy: 96.61\n",
      "correct: 32522, samples: 33664, accuracy: 96.61\n",
      "correct: 32583, samples: 33728, accuracy: 96.61\n",
      "correct: 32645, samples: 33792, accuracy: 96.61\n",
      "correct: 32706, samples: 33856, accuracy: 96.60\n",
      "correct: 32769, samples: 33920, accuracy: 96.61\n",
      "correct: 32832, samples: 33984, accuracy: 96.61\n",
      "correct: 32895, samples: 34048, accuracy: 96.61\n",
      "correct: 32958, samples: 34112, accuracy: 96.62\n",
      "correct: 33020, samples: 34176, accuracy: 96.62\n",
      "correct: 33083, samples: 34240, accuracy: 96.62\n",
      "correct: 33143, samples: 34304, accuracy: 96.62\n",
      "correct: 33206, samples: 34368, accuracy: 96.62\n",
      "correct: 33267, samples: 34432, accuracy: 96.62\n",
      "correct: 33328, samples: 34496, accuracy: 96.61\n",
      "correct: 33390, samples: 34560, accuracy: 96.61\n",
      "correct: 33454, samples: 34624, accuracy: 96.62\n",
      "correct: 33515, samples: 34688, accuracy: 96.62\n",
      "correct: 33577, samples: 34752, accuracy: 96.62\n",
      "correct: 33638, samples: 34816, accuracy: 96.62\n",
      "correct: 33700, samples: 34880, accuracy: 96.62\n",
      "correct: 33764, samples: 34944, accuracy: 96.62\n",
      "correct: 33825, samples: 35008, accuracy: 96.62\n",
      "correct: 33887, samples: 35072, accuracy: 96.62\n",
      "correct: 33949, samples: 35136, accuracy: 96.62\n",
      "correct: 34013, samples: 35200, accuracy: 96.63\n",
      "correct: 34077, samples: 35264, accuracy: 96.63\n",
      "correct: 34141, samples: 35328, accuracy: 96.64\n",
      "correct: 34202, samples: 35392, accuracy: 96.64\n",
      "correct: 34264, samples: 35456, accuracy: 96.64\n",
      "correct: 34327, samples: 35520, accuracy: 96.64\n",
      "correct: 34388, samples: 35584, accuracy: 96.64\n",
      "correct: 34451, samples: 35648, accuracy: 96.64\n",
      "correct: 34511, samples: 35712, accuracy: 96.64\n",
      "correct: 34575, samples: 35776, accuracy: 96.64\n",
      "correct: 34636, samples: 35840, accuracy: 96.64\n",
      "correct: 34696, samples: 35904, accuracy: 96.64\n",
      "correct: 34759, samples: 35968, accuracy: 96.64\n",
      "correct: 34819, samples: 36032, accuracy: 96.63\n",
      "correct: 34877, samples: 36096, accuracy: 96.62\n",
      "correct: 34936, samples: 36160, accuracy: 96.62\n",
      "correct: 34997, samples: 36224, accuracy: 96.61\n",
      "correct: 35060, samples: 36288, accuracy: 96.62\n",
      "correct: 35123, samples: 36352, accuracy: 96.62\n",
      "correct: 35186, samples: 36416, accuracy: 96.62\n",
      "correct: 35248, samples: 36480, accuracy: 96.62\n",
      "correct: 35311, samples: 36544, accuracy: 96.63\n",
      "correct: 35372, samples: 36608, accuracy: 96.62\n",
      "correct: 35430, samples: 36672, accuracy: 96.61\n",
      "correct: 35491, samples: 36736, accuracy: 96.61\n",
      "correct: 35554, samples: 36800, accuracy: 96.61\n",
      "correct: 35617, samples: 36864, accuracy: 96.62\n",
      "correct: 35681, samples: 36928, accuracy: 96.62\n",
      "correct: 35742, samples: 36992, accuracy: 96.62\n",
      "correct: 35805, samples: 37056, accuracy: 96.62\n",
      "correct: 35866, samples: 37120, accuracy: 96.62\n",
      "correct: 35928, samples: 37184, accuracy: 96.62\n",
      "correct: 35991, samples: 37248, accuracy: 96.63\n",
      "correct: 36055, samples: 37312, accuracy: 96.63\n",
      "correct: 36118, samples: 37376, accuracy: 96.63\n",
      "correct: 36177, samples: 37440, accuracy: 96.63\n",
      "correct: 36240, samples: 37504, accuracy: 96.63\n",
      "correct: 36302, samples: 37568, accuracy: 96.63\n",
      "correct: 36361, samples: 37632, accuracy: 96.62\n",
      "correct: 36424, samples: 37696, accuracy: 96.63\n",
      "correct: 36487, samples: 37760, accuracy: 96.63\n",
      "correct: 36550, samples: 37824, accuracy: 96.63\n",
      "correct: 36609, samples: 37888, accuracy: 96.62\n",
      "correct: 36669, samples: 37952, accuracy: 96.62\n",
      "correct: 36733, samples: 38016, accuracy: 96.63\n",
      "correct: 36795, samples: 38080, accuracy: 96.63\n",
      "correct: 36856, samples: 38144, accuracy: 96.62\n",
      "correct: 36917, samples: 38208, accuracy: 96.62\n",
      "correct: 36978, samples: 38272, accuracy: 96.62\n",
      "correct: 37042, samples: 38336, accuracy: 96.62\n",
      "correct: 37105, samples: 38400, accuracy: 96.63\n",
      "correct: 37165, samples: 38464, accuracy: 96.62\n",
      "correct: 37226, samples: 38528, accuracy: 96.62\n",
      "correct: 37288, samples: 38592, accuracy: 96.62\n",
      "correct: 37349, samples: 38656, accuracy: 96.62\n",
      "correct: 37412, samples: 38720, accuracy: 96.62\n",
      "correct: 37473, samples: 38784, accuracy: 96.62\n",
      "correct: 37535, samples: 38848, accuracy: 96.62\n",
      "correct: 37598, samples: 38912, accuracy: 96.62\n",
      "correct: 37661, samples: 38976, accuracy: 96.63\n",
      "correct: 37725, samples: 39040, accuracy: 96.63\n",
      "correct: 37787, samples: 39104, accuracy: 96.63\n",
      "correct: 37849, samples: 39168, accuracy: 96.63\n",
      "correct: 37913, samples: 39232, accuracy: 96.64\n",
      "correct: 37976, samples: 39296, accuracy: 96.64\n",
      "correct: 38037, samples: 39360, accuracy: 96.64\n",
      "correct: 38101, samples: 39424, accuracy: 96.64\n",
      "correct: 38159, samples: 39488, accuracy: 96.63\n",
      "correct: 38221, samples: 39552, accuracy: 96.63\n",
      "correct: 38284, samples: 39616, accuracy: 96.64\n",
      "correct: 38347, samples: 39680, accuracy: 96.64\n",
      "correct: 38409, samples: 39744, accuracy: 96.64\n",
      "correct: 38470, samples: 39808, accuracy: 96.64\n",
      "correct: 38531, samples: 39872, accuracy: 96.64\n",
      "correct: 38594, samples: 39936, accuracy: 96.64\n",
      "correct: 38655, samples: 40000, accuracy: 96.64\n",
      "correct: 38714, samples: 40064, accuracy: 96.63\n",
      "correct: 38778, samples: 40128, accuracy: 96.64\n",
      "correct: 38841, samples: 40192, accuracy: 96.64\n",
      "correct: 38905, samples: 40256, accuracy: 96.64\n",
      "correct: 38969, samples: 40320, accuracy: 96.65\n",
      "correct: 39031, samples: 40384, accuracy: 96.65\n",
      "correct: 39093, samples: 40448, accuracy: 96.65\n",
      "correct: 39153, samples: 40512, accuracy: 96.65\n",
      "correct: 39216, samples: 40576, accuracy: 96.65\n",
      "correct: 39280, samples: 40640, accuracy: 96.65\n",
      "correct: 39339, samples: 40704, accuracy: 96.65\n",
      "correct: 39403, samples: 40768, accuracy: 96.65\n",
      "correct: 39466, samples: 40832, accuracy: 96.65\n",
      "correct: 39528, samples: 40896, accuracy: 96.65\n",
      "correct: 39589, samples: 40960, accuracy: 96.65\n",
      "correct: 39651, samples: 41024, accuracy: 96.65\n",
      "correct: 39712, samples: 41088, accuracy: 96.65\n",
      "correct: 39773, samples: 41152, accuracy: 96.65\n",
      "correct: 39835, samples: 41216, accuracy: 96.65\n",
      "correct: 39897, samples: 41280, accuracy: 96.65\n",
      "correct: 39960, samples: 41344, accuracy: 96.65\n",
      "correct: 40022, samples: 41408, accuracy: 96.65\n",
      "correct: 40084, samples: 41472, accuracy: 96.65\n",
      "correct: 40147, samples: 41536, accuracy: 96.66\n",
      "correct: 40208, samples: 41600, accuracy: 96.65\n",
      "correct: 40272, samples: 41664, accuracy: 96.66\n",
      "correct: 40335, samples: 41728, accuracy: 96.66\n",
      "correct: 40397, samples: 41792, accuracy: 96.66\n",
      "correct: 40460, samples: 41856, accuracy: 96.66\n",
      "correct: 40523, samples: 41920, accuracy: 96.67\n",
      "correct: 40585, samples: 41984, accuracy: 96.67\n",
      "correct: 40647, samples: 42048, accuracy: 96.67\n",
      "correct: 40709, samples: 42112, accuracy: 96.67\n",
      "correct: 40773, samples: 42176, accuracy: 96.67\n",
      "correct: 40836, samples: 42240, accuracy: 96.68\n",
      "correct: 40899, samples: 42304, accuracy: 96.68\n",
      "correct: 40960, samples: 42368, accuracy: 96.68\n",
      "correct: 41023, samples: 42432, accuracy: 96.68\n",
      "correct: 41083, samples: 42496, accuracy: 96.67\n",
      "correct: 41147, samples: 42560, accuracy: 96.68\n",
      "correct: 41208, samples: 42624, accuracy: 96.68\n",
      "correct: 41270, samples: 42688, accuracy: 96.68\n",
      "correct: 41330, samples: 42752, accuracy: 96.67\n",
      "correct: 41394, samples: 42816, accuracy: 96.68\n",
      "correct: 41455, samples: 42880, accuracy: 96.68\n",
      "correct: 41518, samples: 42944, accuracy: 96.68\n",
      "correct: 41581, samples: 43008, accuracy: 96.68\n",
      "correct: 41642, samples: 43072, accuracy: 96.68\n",
      "correct: 41704, samples: 43136, accuracy: 96.68\n",
      "correct: 41767, samples: 43200, accuracy: 96.68\n",
      "correct: 41828, samples: 43264, accuracy: 96.68\n",
      "correct: 41891, samples: 43328, accuracy: 96.68\n",
      "correct: 41952, samples: 43392, accuracy: 96.68\n",
      "correct: 42013, samples: 43456, accuracy: 96.68\n",
      "correct: 42077, samples: 43520, accuracy: 96.68\n",
      "correct: 42140, samples: 43584, accuracy: 96.69\n",
      "correct: 42204, samples: 43648, accuracy: 96.69\n",
      "correct: 42264, samples: 43712, accuracy: 96.69\n",
      "correct: 42325, samples: 43776, accuracy: 96.69\n",
      "correct: 42386, samples: 43840, accuracy: 96.68\n",
      "correct: 42449, samples: 43904, accuracy: 96.69\n",
      "correct: 42512, samples: 43968, accuracy: 96.69\n",
      "correct: 42574, samples: 44032, accuracy: 96.69\n",
      "correct: 42636, samples: 44096, accuracy: 96.69\n",
      "correct: 42697, samples: 44160, accuracy: 96.69\n",
      "correct: 42759, samples: 44224, accuracy: 96.69\n",
      "correct: 42819, samples: 44288, accuracy: 96.68\n",
      "correct: 42880, samples: 44352, accuracy: 96.68\n",
      "correct: 42943, samples: 44416, accuracy: 96.68\n",
      "correct: 43007, samples: 44480, accuracy: 96.69\n",
      "correct: 43068, samples: 44544, accuracy: 96.69\n",
      "correct: 43131, samples: 44608, accuracy: 96.69\n",
      "correct: 43192, samples: 44672, accuracy: 96.69\n",
      "correct: 43255, samples: 44736, accuracy: 96.69\n",
      "correct: 43318, samples: 44800, accuracy: 96.69\n",
      "correct: 43379, samples: 44864, accuracy: 96.69\n",
      "correct: 43443, samples: 44928, accuracy: 96.69\n",
      "correct: 43506, samples: 44992, accuracy: 96.70\n",
      "correct: 43567, samples: 45056, accuracy: 96.70\n",
      "correct: 43629, samples: 45120, accuracy: 96.70\n",
      "correct: 43692, samples: 45184, accuracy: 96.70\n",
      "correct: 43753, samples: 45248, accuracy: 96.70\n",
      "correct: 43813, samples: 45312, accuracy: 96.69\n",
      "correct: 43874, samples: 45376, accuracy: 96.69\n",
      "correct: 43937, samples: 45440, accuracy: 96.69\n",
      "correct: 43996, samples: 45504, accuracy: 96.69\n",
      "correct: 44060, samples: 45568, accuracy: 96.69\n",
      "correct: 44122, samples: 45632, accuracy: 96.69\n",
      "correct: 44185, samples: 45696, accuracy: 96.69\n",
      "correct: 44248, samples: 45760, accuracy: 96.70\n",
      "correct: 44308, samples: 45824, accuracy: 96.69\n",
      "correct: 44368, samples: 45888, accuracy: 96.69\n",
      "correct: 44430, samples: 45952, accuracy: 96.69\n",
      "correct: 44493, samples: 46016, accuracy: 96.69\n",
      "correct: 44555, samples: 46080, accuracy: 96.69\n",
      "correct: 44619, samples: 46144, accuracy: 96.70\n",
      "correct: 44683, samples: 46208, accuracy: 96.70\n",
      "correct: 44747, samples: 46272, accuracy: 96.70\n",
      "correct: 44811, samples: 46336, accuracy: 96.71\n",
      "correct: 44871, samples: 46400, accuracy: 96.70\n",
      "correct: 44934, samples: 46464, accuracy: 96.71\n",
      "correct: 44996, samples: 46528, accuracy: 96.71\n",
      "correct: 45058, samples: 46592, accuracy: 96.71\n",
      "correct: 45118, samples: 46656, accuracy: 96.70\n",
      "correct: 45180, samples: 46720, accuracy: 96.70\n",
      "correct: 45241, samples: 46784, accuracy: 96.70\n",
      "correct: 45298, samples: 46848, accuracy: 96.69\n",
      "correct: 45361, samples: 46912, accuracy: 96.69\n",
      "correct: 45422, samples: 46976, accuracy: 96.69\n",
      "correct: 45483, samples: 47040, accuracy: 96.69\n",
      "correct: 45544, samples: 47104, accuracy: 96.69\n",
      "correct: 45605, samples: 47168, accuracy: 96.69\n",
      "correct: 45667, samples: 47232, accuracy: 96.69\n",
      "correct: 45729, samples: 47296, accuracy: 96.69\n",
      "correct: 45792, samples: 47360, accuracy: 96.69\n",
      "correct: 45854, samples: 47424, accuracy: 96.69\n",
      "correct: 45915, samples: 47488, accuracy: 96.69\n",
      "correct: 45977, samples: 47552, accuracy: 96.69\n",
      "correct: 46041, samples: 47616, accuracy: 96.69\n",
      "correct: 46102, samples: 47680, accuracy: 96.69\n",
      "correct: 46163, samples: 47744, accuracy: 96.69\n",
      "correct: 46223, samples: 47808, accuracy: 96.68\n",
      "correct: 46284, samples: 47872, accuracy: 96.68\n",
      "correct: 46347, samples: 47936, accuracy: 96.69\n",
      "correct: 46409, samples: 48000, accuracy: 96.69\n",
      "correct: 46472, samples: 48064, accuracy: 96.69\n",
      "correct: 46534, samples: 48128, accuracy: 96.69\n",
      "correct: 46594, samples: 48192, accuracy: 96.68\n",
      "correct: 46654, samples: 48256, accuracy: 96.68\n",
      "correct: 46717, samples: 48320, accuracy: 96.68\n",
      "correct: 46781, samples: 48384, accuracy: 96.69\n",
      "correct: 46842, samples: 48448, accuracy: 96.69\n",
      "correct: 46903, samples: 48512, accuracy: 96.68\n",
      "correct: 46965, samples: 48576, accuracy: 96.68\n",
      "correct: 47026, samples: 48640, accuracy: 96.68\n",
      "correct: 47089, samples: 48704, accuracy: 96.68\n",
      "correct: 47152, samples: 48768, accuracy: 96.69\n",
      "correct: 47214, samples: 48832, accuracy: 96.69\n",
      "correct: 47278, samples: 48896, accuracy: 96.69\n",
      "correct: 47337, samples: 48960, accuracy: 96.69\n",
      "correct: 47400, samples: 49024, accuracy: 96.69\n",
      "correct: 47460, samples: 49088, accuracy: 96.68\n",
      "correct: 47524, samples: 49152, accuracy: 96.69\n",
      "correct: 47584, samples: 49216, accuracy: 96.68\n",
      "correct: 47646, samples: 49280, accuracy: 96.68\n",
      "correct: 47707, samples: 49344, accuracy: 96.68\n",
      "correct: 47770, samples: 49408, accuracy: 96.68\n",
      "correct: 47831, samples: 49472, accuracy: 96.68\n",
      "correct: 47891, samples: 49536, accuracy: 96.68\n",
      "correct: 47952, samples: 49600, accuracy: 96.68\n",
      "correct: 48011, samples: 49664, accuracy: 96.67\n",
      "correct: 48073, samples: 49728, accuracy: 96.67\n",
      "correct: 48136, samples: 49792, accuracy: 96.67\n",
      "correct: 48200, samples: 49856, accuracy: 96.68\n",
      "correct: 48258, samples: 49920, accuracy: 96.67\n",
      "correct: 48320, samples: 49984, accuracy: 96.67\n",
      "correct: 48381, samples: 50048, accuracy: 96.67\n",
      "correct: 48443, samples: 50112, accuracy: 96.67\n",
      "correct: 48504, samples: 50176, accuracy: 96.67\n",
      "correct: 48564, samples: 50240, accuracy: 96.66\n",
      "correct: 48624, samples: 50304, accuracy: 96.66\n",
      "correct: 48686, samples: 50368, accuracy: 96.66\n",
      "correct: 48749, samples: 50432, accuracy: 96.66\n",
      "correct: 48813, samples: 50496, accuracy: 96.67\n",
      "correct: 48873, samples: 50560, accuracy: 96.66\n",
      "correct: 48934, samples: 50624, accuracy: 96.66\n",
      "correct: 48996, samples: 50688, accuracy: 96.66\n",
      "correct: 49058, samples: 50752, accuracy: 96.66\n",
      "correct: 49122, samples: 50816, accuracy: 96.67\n",
      "correct: 49185, samples: 50880, accuracy: 96.67\n",
      "correct: 49248, samples: 50944, accuracy: 96.67\n",
      "correct: 49309, samples: 51008, accuracy: 96.67\n",
      "correct: 49371, samples: 51072, accuracy: 96.67\n",
      "correct: 49432, samples: 51136, accuracy: 96.67\n",
      "correct: 49494, samples: 51200, accuracy: 96.67\n",
      "correct: 49556, samples: 51264, accuracy: 96.67\n",
      "correct: 49617, samples: 51328, accuracy: 96.67\n",
      "correct: 49678, samples: 51392, accuracy: 96.66\n",
      "correct: 49741, samples: 51456, accuracy: 96.67\n",
      "correct: 49803, samples: 51520, accuracy: 96.67\n",
      "correct: 49866, samples: 51584, accuracy: 96.67\n",
      "correct: 49928, samples: 51648, accuracy: 96.67\n",
      "correct: 49989, samples: 51712, accuracy: 96.67\n",
      "correct: 50052, samples: 51776, accuracy: 96.67\n",
      "correct: 50114, samples: 51840, accuracy: 96.67\n",
      "correct: 50176, samples: 51904, accuracy: 96.67\n",
      "correct: 50239, samples: 51968, accuracy: 96.67\n",
      "correct: 50300, samples: 52032, accuracy: 96.67\n",
      "correct: 50359, samples: 52096, accuracy: 96.67\n",
      "correct: 50419, samples: 52160, accuracy: 96.66\n",
      "correct: 50481, samples: 52224, accuracy: 96.66\n",
      "correct: 50544, samples: 52288, accuracy: 96.66\n",
      "correct: 50606, samples: 52352, accuracy: 96.66\n",
      "correct: 50669, samples: 52416, accuracy: 96.67\n",
      "correct: 50732, samples: 52480, accuracy: 96.67\n",
      "correct: 50795, samples: 52544, accuracy: 96.67\n",
      "correct: 50855, samples: 52608, accuracy: 96.67\n",
      "correct: 50918, samples: 52672, accuracy: 96.67\n",
      "correct: 50982, samples: 52736, accuracy: 96.67\n",
      "correct: 51041, samples: 52800, accuracy: 96.67\n",
      "correct: 51104, samples: 52864, accuracy: 96.67\n",
      "correct: 51168, samples: 52928, accuracy: 96.67\n",
      "correct: 51232, samples: 52992, accuracy: 96.68\n",
      "correct: 51293, samples: 53056, accuracy: 96.68\n",
      "correct: 51353, samples: 53120, accuracy: 96.67\n",
      "correct: 51417, samples: 53184, accuracy: 96.68\n",
      "correct: 51476, samples: 53248, accuracy: 96.67\n",
      "correct: 51538, samples: 53312, accuracy: 96.67\n",
      "correct: 51599, samples: 53376, accuracy: 96.67\n",
      "correct: 51661, samples: 53440, accuracy: 96.67\n",
      "correct: 51722, samples: 53504, accuracy: 96.67\n",
      "correct: 51782, samples: 53568, accuracy: 96.67\n",
      "correct: 51846, samples: 53632, accuracy: 96.67\n",
      "correct: 51906, samples: 53696, accuracy: 96.67\n",
      "correct: 51966, samples: 53760, accuracy: 96.66\n",
      "correct: 52029, samples: 53824, accuracy: 96.67\n",
      "correct: 52091, samples: 53888, accuracy: 96.67\n",
      "correct: 52154, samples: 53952, accuracy: 96.67\n",
      "correct: 52217, samples: 54016, accuracy: 96.67\n",
      "correct: 52278, samples: 54080, accuracy: 96.67\n",
      "correct: 52339, samples: 54144, accuracy: 96.67\n",
      "correct: 52402, samples: 54208, accuracy: 96.67\n",
      "correct: 52463, samples: 54272, accuracy: 96.67\n",
      "correct: 52524, samples: 54336, accuracy: 96.67\n",
      "correct: 52587, samples: 54400, accuracy: 96.67\n",
      "correct: 52648, samples: 54464, accuracy: 96.67\n",
      "correct: 52712, samples: 54528, accuracy: 96.67\n",
      "correct: 52774, samples: 54592, accuracy: 96.67\n",
      "correct: 52835, samples: 54656, accuracy: 96.67\n",
      "correct: 52895, samples: 54720, accuracy: 96.66\n",
      "correct: 52957, samples: 54784, accuracy: 96.67\n",
      "correct: 53018, samples: 54848, accuracy: 96.66\n",
      "correct: 53082, samples: 54912, accuracy: 96.67\n",
      "correct: 53143, samples: 54976, accuracy: 96.67\n",
      "correct: 53203, samples: 55040, accuracy: 96.66\n",
      "correct: 53266, samples: 55104, accuracy: 96.66\n",
      "correct: 53325, samples: 55168, accuracy: 96.66\n",
      "correct: 53384, samples: 55232, accuracy: 96.65\n",
      "correct: 53446, samples: 55296, accuracy: 96.65\n",
      "correct: 53508, samples: 55360, accuracy: 96.65\n",
      "correct: 53572, samples: 55424, accuracy: 96.66\n",
      "correct: 53629, samples: 55488, accuracy: 96.65\n",
      "correct: 53691, samples: 55552, accuracy: 96.65\n",
      "correct: 53753, samples: 55616, accuracy: 96.65\n",
      "correct: 53815, samples: 55680, accuracy: 96.65\n",
      "correct: 53875, samples: 55744, accuracy: 96.65\n",
      "correct: 53939, samples: 55808, accuracy: 96.65\n",
      "correct: 54000, samples: 55872, accuracy: 96.65\n",
      "correct: 54062, samples: 55936, accuracy: 96.65\n",
      "correct: 54124, samples: 56000, accuracy: 96.65\n",
      "correct: 54187, samples: 56064, accuracy: 96.65\n",
      "correct: 54246, samples: 56128, accuracy: 96.65\n",
      "correct: 54309, samples: 56192, accuracy: 96.65\n",
      "correct: 54372, samples: 56256, accuracy: 96.65\n",
      "correct: 54435, samples: 56320, accuracy: 96.65\n",
      "correct: 54497, samples: 56384, accuracy: 96.65\n",
      "correct: 54557, samples: 56448, accuracy: 96.65\n",
      "correct: 54621, samples: 56512, accuracy: 96.65\n",
      "correct: 54682, samples: 56576, accuracy: 96.65\n",
      "correct: 54743, samples: 56640, accuracy: 96.65\n",
      "correct: 54806, samples: 56704, accuracy: 96.65\n",
      "correct: 54869, samples: 56768, accuracy: 96.65\n",
      "correct: 54929, samples: 56832, accuracy: 96.65\n",
      "correct: 54988, samples: 56896, accuracy: 96.65\n",
      "correct: 55047, samples: 56960, accuracy: 96.64\n",
      "correct: 55107, samples: 57024, accuracy: 96.64\n",
      "correct: 55169, samples: 57088, accuracy: 96.64\n",
      "correct: 55231, samples: 57152, accuracy: 96.64\n",
      "correct: 55292, samples: 57216, accuracy: 96.64\n",
      "correct: 55353, samples: 57280, accuracy: 96.64\n",
      "correct: 55415, samples: 57344, accuracy: 96.64\n",
      "correct: 55477, samples: 57408, accuracy: 96.64\n",
      "correct: 55539, samples: 57472, accuracy: 96.64\n",
      "correct: 55603, samples: 57536, accuracy: 96.64\n",
      "correct: 55666, samples: 57600, accuracy: 96.64\n",
      "correct: 55726, samples: 57664, accuracy: 96.64\n",
      "correct: 55788, samples: 57728, accuracy: 96.64\n",
      "correct: 55851, samples: 57792, accuracy: 96.64\n",
      "correct: 55914, samples: 57856, accuracy: 96.64\n",
      "correct: 55974, samples: 57920, accuracy: 96.64\n",
      "correct: 56036, samples: 57984, accuracy: 96.64\n",
      "correct: 56098, samples: 58048, accuracy: 96.64\n",
      "correct: 56161, samples: 58112, accuracy: 96.64\n",
      "correct: 56225, samples: 58176, accuracy: 96.65\n",
      "correct: 56288, samples: 58240, accuracy: 96.65\n",
      "correct: 56348, samples: 58304, accuracy: 96.65\n",
      "correct: 56406, samples: 58368, accuracy: 96.64\n",
      "correct: 56465, samples: 58432, accuracy: 96.63\n",
      "correct: 56527, samples: 58496, accuracy: 96.63\n",
      "correct: 56590, samples: 58560, accuracy: 96.64\n",
      "correct: 56651, samples: 58624, accuracy: 96.63\n",
      "correct: 56712, samples: 58688, accuracy: 96.63\n",
      "correct: 56776, samples: 58752, accuracy: 96.64\n",
      "correct: 56839, samples: 58816, accuracy: 96.64\n",
      "correct: 56901, samples: 58880, accuracy: 96.64\n",
      "correct: 56963, samples: 58944, accuracy: 96.64\n",
      "correct: 57026, samples: 59008, accuracy: 96.64\n",
      "correct: 57087, samples: 59072, accuracy: 96.64\n",
      "correct: 57151, samples: 59136, accuracy: 96.64\n",
      "correct: 57213, samples: 59200, accuracy: 96.64\n",
      "correct: 57273, samples: 59264, accuracy: 96.64\n",
      "correct: 57336, samples: 59328, accuracy: 96.64\n",
      "correct: 57395, samples: 59392, accuracy: 96.64\n",
      "correct: 57459, samples: 59456, accuracy: 96.64\n",
      "correct: 57522, samples: 59520, accuracy: 96.64\n",
      "correct: 57584, samples: 59584, accuracy: 96.64\n",
      "correct: 57647, samples: 59648, accuracy: 96.65\n",
      "correct: 57710, samples: 59712, accuracy: 96.65\n",
      "correct: 57772, samples: 59776, accuracy: 96.65\n",
      "correct: 57835, samples: 59840, accuracy: 96.65\n",
      "correct: 57897, samples: 59904, accuracy: 96.65\n",
      "correct: 57959, samples: 59968, accuracy: 96.65\n",
      "correct: 57989, samples: 60000, accuracy: 96.65\n",
      "test data accuracy\n",
      "correct: 61, samples: 64, accuracy: 95.31\n",
      "correct: 123, samples: 128, accuracy: 96.09\n",
      "correct: 186, samples: 192, accuracy: 96.88\n",
      "correct: 247, samples: 256, accuracy: 96.48\n",
      "correct: 305, samples: 320, accuracy: 95.31\n",
      "correct: 365, samples: 384, accuracy: 95.05\n",
      "correct: 429, samples: 448, accuracy: 95.76\n",
      "correct: 490, samples: 512, accuracy: 95.70\n",
      "correct: 553, samples: 576, accuracy: 96.01\n",
      "correct: 611, samples: 640, accuracy: 95.47\n",
      "correct: 675, samples: 704, accuracy: 95.88\n",
      "correct: 735, samples: 768, accuracy: 95.70\n",
      "correct: 798, samples: 832, accuracy: 95.91\n",
      "correct: 857, samples: 896, accuracy: 95.65\n",
      "correct: 920, samples: 960, accuracy: 95.83\n",
      "correct: 982, samples: 1024, accuracy: 95.90\n",
      "correct: 1044, samples: 1088, accuracy: 95.96\n",
      "correct: 1104, samples: 1152, accuracy: 95.83\n",
      "correct: 1168, samples: 1216, accuracy: 96.05\n",
      "correct: 1229, samples: 1280, accuracy: 96.02\n",
      "correct: 1288, samples: 1344, accuracy: 95.83\n",
      "correct: 1349, samples: 1408, accuracy: 95.81\n",
      "correct: 1412, samples: 1472, accuracy: 95.92\n",
      "correct: 1471, samples: 1536, accuracy: 95.77\n",
      "correct: 1534, samples: 1600, accuracy: 95.88\n",
      "correct: 1597, samples: 1664, accuracy: 95.97\n",
      "correct: 1661, samples: 1728, accuracy: 96.12\n",
      "correct: 1723, samples: 1792, accuracy: 96.15\n",
      "correct: 1784, samples: 1856, accuracy: 96.12\n",
      "correct: 1846, samples: 1920, accuracy: 96.15\n",
      "correct: 1909, samples: 1984, accuracy: 96.22\n",
      "correct: 1969, samples: 2048, accuracy: 96.14\n",
      "correct: 2031, samples: 2112, accuracy: 96.16\n",
      "correct: 2093, samples: 2176, accuracy: 96.19\n",
      "correct: 2154, samples: 2240, accuracy: 96.16\n",
      "correct: 2216, samples: 2304, accuracy: 96.18\n",
      "correct: 2273, samples: 2368, accuracy: 95.99\n",
      "correct: 2335, samples: 2432, accuracy: 96.01\n",
      "correct: 2396, samples: 2496, accuracy: 95.99\n",
      "correct: 2457, samples: 2560, accuracy: 95.98\n",
      "correct: 2518, samples: 2624, accuracy: 95.96\n",
      "correct: 2578, samples: 2688, accuracy: 95.91\n",
      "correct: 2640, samples: 2752, accuracy: 95.93\n",
      "correct: 2703, samples: 2816, accuracy: 95.99\n",
      "correct: 2762, samples: 2880, accuracy: 95.90\n",
      "correct: 2822, samples: 2944, accuracy: 95.86\n",
      "correct: 2882, samples: 3008, accuracy: 95.81\n",
      "correct: 2942, samples: 3072, accuracy: 95.77\n",
      "correct: 3001, samples: 3136, accuracy: 95.70\n",
      "correct: 3064, samples: 3200, accuracy: 95.75\n",
      "correct: 3122, samples: 3264, accuracy: 95.65\n",
      "correct: 3185, samples: 3328, accuracy: 95.70\n",
      "correct: 3248, samples: 3392, accuracy: 95.75\n",
      "correct: 3311, samples: 3456, accuracy: 95.80\n",
      "correct: 3372, samples: 3520, accuracy: 95.80\n",
      "correct: 3434, samples: 3584, accuracy: 95.81\n",
      "correct: 3498, samples: 3648, accuracy: 95.89\n",
      "correct: 3561, samples: 3712, accuracy: 95.93\n",
      "correct: 3623, samples: 3776, accuracy: 95.95\n",
      "correct: 3684, samples: 3840, accuracy: 95.94\n",
      "correct: 3746, samples: 3904, accuracy: 95.95\n",
      "correct: 3806, samples: 3968, accuracy: 95.92\n",
      "correct: 3865, samples: 4032, accuracy: 95.86\n",
      "correct: 3927, samples: 4096, accuracy: 95.87\n",
      "correct: 3988, samples: 4160, accuracy: 95.87\n",
      "correct: 4048, samples: 4224, accuracy: 95.83\n",
      "correct: 4108, samples: 4288, accuracy: 95.80\n",
      "correct: 4169, samples: 4352, accuracy: 95.80\n",
      "correct: 4232, samples: 4416, accuracy: 95.83\n",
      "correct: 4291, samples: 4480, accuracy: 95.78\n",
      "correct: 4355, samples: 4544, accuracy: 95.84\n",
      "correct: 4417, samples: 4608, accuracy: 95.86\n",
      "correct: 4477, samples: 4672, accuracy: 95.83\n",
      "correct: 4537, samples: 4736, accuracy: 95.80\n",
      "correct: 4596, samples: 4800, accuracy: 95.75\n",
      "correct: 4657, samples: 4864, accuracy: 95.74\n",
      "correct: 4717, samples: 4928, accuracy: 95.72\n",
      "correct: 4778, samples: 4992, accuracy: 95.71\n",
      "correct: 4841, samples: 5056, accuracy: 95.75\n",
      "correct: 4902, samples: 5120, accuracy: 95.74\n",
      "correct: 4965, samples: 5184, accuracy: 95.78\n",
      "correct: 5027, samples: 5248, accuracy: 95.79\n",
      "correct: 5091, samples: 5312, accuracy: 95.84\n",
      "correct: 5153, samples: 5376, accuracy: 95.85\n",
      "correct: 5213, samples: 5440, accuracy: 95.83\n",
      "correct: 5276, samples: 5504, accuracy: 95.86\n",
      "correct: 5338, samples: 5568, accuracy: 95.87\n",
      "correct: 5397, samples: 5632, accuracy: 95.83\n",
      "correct: 5458, samples: 5696, accuracy: 95.82\n",
      "correct: 5520, samples: 5760, accuracy: 95.83\n",
      "correct: 5578, samples: 5824, accuracy: 95.78\n",
      "correct: 5642, samples: 5888, accuracy: 95.82\n",
      "correct: 5700, samples: 5952, accuracy: 95.77\n",
      "correct: 5762, samples: 6016, accuracy: 95.78\n",
      "correct: 5823, samples: 6080, accuracy: 95.77\n",
      "correct: 5886, samples: 6144, accuracy: 95.80\n",
      "correct: 5948, samples: 6208, accuracy: 95.81\n",
      "correct: 6007, samples: 6272, accuracy: 95.77\n",
      "correct: 6067, samples: 6336, accuracy: 95.75\n",
      "correct: 6130, samples: 6400, accuracy: 95.78\n",
      "correct: 6191, samples: 6464, accuracy: 95.78\n",
      "correct: 6251, samples: 6528, accuracy: 95.76\n",
      "correct: 6314, samples: 6592, accuracy: 95.78\n",
      "correct: 6374, samples: 6656, accuracy: 95.76\n",
      "correct: 6431, samples: 6720, accuracy: 95.70\n",
      "correct: 6493, samples: 6784, accuracy: 95.71\n",
      "correct: 6555, samples: 6848, accuracy: 95.72\n",
      "correct: 6619, samples: 6912, accuracy: 95.76\n",
      "correct: 6679, samples: 6976, accuracy: 95.74\n",
      "correct: 6738, samples: 7040, accuracy: 95.71\n",
      "correct: 6799, samples: 7104, accuracy: 95.71\n",
      "correct: 6861, samples: 7168, accuracy: 95.72\n",
      "correct: 6921, samples: 7232, accuracy: 95.70\n",
      "correct: 6983, samples: 7296, accuracy: 95.71\n",
      "correct: 7045, samples: 7360, accuracy: 95.72\n",
      "correct: 7107, samples: 7424, accuracy: 95.73\n",
      "correct: 7168, samples: 7488, accuracy: 95.73\n",
      "correct: 7229, samples: 7552, accuracy: 95.72\n",
      "correct: 7292, samples: 7616, accuracy: 95.75\n",
      "correct: 7356, samples: 7680, accuracy: 95.78\n",
      "correct: 7418, samples: 7744, accuracy: 95.79\n",
      "correct: 7478, samples: 7808, accuracy: 95.77\n",
      "correct: 7538, samples: 7872, accuracy: 95.76\n",
      "correct: 7600, samples: 7936, accuracy: 95.77\n",
      "correct: 7663, samples: 8000, accuracy: 95.79\n",
      "correct: 7726, samples: 8064, accuracy: 95.81\n",
      "correct: 7787, samples: 8128, accuracy: 95.80\n",
      "correct: 7848, samples: 8192, accuracy: 95.80\n",
      "correct: 7910, samples: 8256, accuracy: 95.81\n",
      "correct: 7972, samples: 8320, accuracy: 95.82\n",
      "correct: 8036, samples: 8384, accuracy: 95.85\n",
      "correct: 8098, samples: 8448, accuracy: 95.86\n",
      "correct: 8160, samples: 8512, accuracy: 95.86\n",
      "correct: 8220, samples: 8576, accuracy: 95.85\n",
      "correct: 8279, samples: 8640, accuracy: 95.82\n",
      "correct: 8341, samples: 8704, accuracy: 95.83\n",
      "correct: 8402, samples: 8768, accuracy: 95.83\n",
      "correct: 8461, samples: 8832, accuracy: 95.80\n",
      "correct: 8525, samples: 8896, accuracy: 95.83\n",
      "correct: 8587, samples: 8960, accuracy: 95.84\n",
      "correct: 8649, samples: 9024, accuracy: 95.84\n",
      "correct: 8709, samples: 9088, accuracy: 95.83\n",
      "correct: 8771, samples: 9152, accuracy: 95.84\n",
      "correct: 8834, samples: 9216, accuracy: 95.86\n",
      "correct: 8892, samples: 9280, accuracy: 95.82\n",
      "correct: 8955, samples: 9344, accuracy: 95.84\n",
      "correct: 9018, samples: 9408, accuracy: 95.85\n",
      "correct: 9078, samples: 9472, accuracy: 95.84\n",
      "correct: 9139, samples: 9536, accuracy: 95.84\n",
      "correct: 9201, samples: 9600, accuracy: 95.84\n",
      "correct: 9263, samples: 9664, accuracy: 95.85\n",
      "correct: 9326, samples: 9728, accuracy: 95.87\n",
      "correct: 9387, samples: 9792, accuracy: 95.86\n",
      "correct: 9449, samples: 9856, accuracy: 95.87\n",
      "correct: 9513, samples: 9920, accuracy: 95.90\n",
      "correct: 9576, samples: 9984, accuracy: 95.91\n",
      "correct: 9592, samples: 10000, accuracy: 95.92\n"
     ]
    }
   ],
   "source": [
    "def check_accuracy(loader:DataLoader, model:NN):\n",
    "    if loader.dataset.train:\n",
    "        print('training data accuracy')\n",
    "    else:\n",
    "        print('test data accuracy')\n",
    "        \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            x = x.reshape(x.shape[0], -1)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, pred = scores.max(1)\n",
    "            num_correct += (pred == y).sum()\n",
    "            num_samples += pred.size(0)\n",
    "        \n",
    "            print(f'correct: {num_correct}, samples: {num_samples}, accuracy: {float(num_correct)/float(num_samples)*100:.2f}')\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "check_accuracy(train_loader, model)\n",
    "check_accuracy(test_loader, model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "22d82af33f03da296223e6f809e18aa0877c44b8d18707f5a9076a6c9918bbd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
